{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"encircle360 OSS Helm Charts","text":"<p>Welcome to the encircle360 OSS Helm Charts repository! This collection provides production-ready open source Helm charts for deploying various applications on Kubernetes, maintained and sponsored by encircle360 GmbH together with the open source community, partners and friends.</p> <p>Documentation: https://encircle360-oss.github.io/helm-charts/docs/</p>"},{"location":"#available-charts","title":"Available Charts","text":"Chart Description Version CNPG Database Manager Multi-database and multi-tenant management for CloudNativePG clusters KubeVirt Virtual Machine Management on Kubernetes Roundcube A free and open source webmail solution with a desktop-like user interface Yaade Open-source, self-hosted, collaborative API development environment"},{"location":"#installation","title":"Installation","text":""},{"location":"#add-the-repository","title":"Add the Repository","text":"<pre><code>helm repo add encircle360 https://encircle360-oss.github.io/helm-charts\nhelm repo update\n</code></pre>"},{"location":"#install-a-chart","title":"Install a Chart","text":"<pre><code># Install Roundcube\nhelm install my-roundcube encircle360/roundcube\n\n# Install CNPG Database Manager\nhelm install my-databases encircle360/cnpg-database-manager\n\n# Install KubeVirt\nhelm install kubevirt encircle360/kubevirt --namespace kubevirt --create-namespace\n\n# Install Yaade\nhelm install my-yaade encircle360/yaade\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>Each chart comes with a <code>values.yaml</code> file that contains the default configuration. You can override these values by:</p> <ol> <li> <p>Using a custom values file: <pre><code>helm install my-release encircle360/chart-name -f my-values.yaml\n</code></pre></p> </li> <li> <p>Using <code>--set</code> flags: <pre><code>helm install my-release encircle360/chart-name --set key=value\n</code></pre></p> </li> </ol>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Kubernetes 1.19+</li> <li>Helm 3.8.0+</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Please see our Contributing Guide for details.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0 - see the LICENSE file for details.</p>"},{"location":"#maintainers-sponsors","title":"Maintainers &amp; Sponsors","text":"<p>This project is maintained and sponsored by encircle360 GmbH, providing enterprise-grade Kubernetes and cloud-native solutions. We work together with our community, partners, and friends to deliver high-quality Helm charts.</p>"},{"location":"#support-professional-services","title":"Support &amp; Professional Services","text":""},{"location":"#community-support","title":"Community Support","text":"<ul> <li>Chart Issues: For Helm chart bugs and feature requests, create an issue</li> <li>General Questions: For questions and discussions, use GitHub Discussions</li> <li>Application Bugs: For bugs within the applications themselves (not chart-related), please report them to the respective upstream project</li> </ul>"},{"location":"#professional-support","title":"Professional Support","text":"<p>For professional support, consulting, custom development, or enterprise solutions, contact us at hello@encircle360.com</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>These Helm charts are provided \"AS IS\" without warranty of any kind. While we strive to maintain high-quality charts and test them thoroughly:</p> <ul> <li>You use these charts at your own risk</li> <li>We recommend thorough testing in non-production environments first</li> <li>Charts may contain bugs or security vulnerabilities</li> <li>We are not liable for any damages or losses resulting from the use of these charts</li> </ul> <p>For production deployments requiring guaranteed support and SLAs, please contact us about our professional services.</p>"},{"location":"#chart-sources","title":"Chart Sources","text":"<p>The source code for all charts can be found in the charts directory of our repository.</p>"},{"location":"charts/cnpg-database-manager/","title":"cnpg-database-manager","text":"<p>Multi-database PostgreSQL management for CloudNativePG with declarative extensions, connection pooling, disaster recovery, and automated backups</p> <p>Homepage: https://github.com/encircle360-oss/helm-charts/tree/main/charts/cnpg-database-manager</p> <p>\u26a0\ufe0f UNDER CONSTRUCTION  This chart is currently under active development and has not been battle-tested in production environments.  NOT PRODUCTION READY - Use at your own risk and thoroughly test in non-production environments first.</p>"},{"location":"charts/cnpg-database-manager/#why-this-chart","title":"Why This Chart?","text":"<p>The official CloudNativePG <code>cluster</code> chart only supports single database per deployment. This chart fills the gap by providing:</p> <ul> <li>Multi-Database Management: Deploy multiple databases in one PostgreSQL cluster</li> <li>Declarative Role Management: Automatic PostgreSQL user/role creation via CloudNativePG managed roles</li> <li>Database CRD Integration: Uses CloudNativePG's Database CRD for proper lifecycle management</li> <li>Multi-Tenant Ready: Perfect for consolidating multiple lightweight databases</li> <li>Flexible Configuration: Support for any CloudNativePG feature via <code>additionalClusterSpec</code></li> </ul> <p>Use this chart when you want to consolidate multiple applications into one PostgreSQL cluster while maintaining security isolation.</p>"},{"location":"charts/cnpg-database-manager/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.24+</li> <li>Helm 3.8+</li> <li>CloudNativePG operator installed (v1.25+)</li> </ul>"},{"location":"charts/cnpg-database-manager/#installing-the-chart","title":"Installing the Chart","text":"<p>To install the chart with the release name <code>my-databases</code>:</p> <pre><code>helm repo add encircle360-oss https://encircle360-oss.github.io/helm-charts/\nhelm repo update\nhelm install my-databases encircle360-oss/cnpg-database-manager\n</code></pre>"},{"location":"charts/cnpg-database-manager/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To uninstall/delete the <code>my-databases</code> deployment:</p> <pre><code>helm uninstall my-databases\n</code></pre>"},{"location":"charts/cnpg-database-manager/#configuration","title":"Configuration","text":""},{"location":"charts/cnpg-database-manager/#basic-example-single-cluster-with-multiple-databases","title":"Basic Example - Single Cluster with Multiple Databases","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n      storageClass: standard\n    databases:\n      - name: keycloak\n        owner: keycloak\n      - name: paperless\n        owner: paperless\n      - name: n8n\n        owner: n8n\n</code></pre> <p>This creates: - One PostgreSQL cluster named <code>main</code> with 3 instances (HA setup) - Three Database CRDs (<code>main-keycloak</code>, <code>main-paperless</code>, <code>main-n8n</code>) - Three managed PostgreSQL roles (<code>keycloak</code>, <code>paperless</code>, <code>n8n</code>) - Three role password secrets (<code>main-keycloak-password</code>, <code>main-paperless-password</code>, <code>main-n8n-password</code>)</p>"},{"location":"charts/cnpg-database-manager/#multi-cluster-setup","title":"Multi-Cluster Setup","text":"<pre><code>clusters:\n  production:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 100Gi\n    databases:\n      - name: api-prod\n        owner: api\n      - name: web-prod\n        owner: web\n\n  staging:\n    enabled: true\n    instances: 1\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 20Gi\n    databases:\n      - name: api-staging\n        owner: api\n      - name: web-staging\n        owner: web\n</code></pre>"},{"location":"charts/cnpg-database-manager/#connection-pooling-pgbouncer","title":"Connection Pooling (PgBouncer)","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    databases:\n      - name: app-db\n        owner: app\n    poolers:\n      - name: rw\n        type: rw  # read-write pooler\n        instances: 3\n        poolMode: transaction\n        parameters:\n          max_client_conn: \"1000\"\n          default_pool_size: \"25\"\n        monitoring:\n          enablePodMonitor: true\n      - name: ro\n        type: ro  # read-only pooler\n        instances: 5\n        poolMode: transaction\n</code></pre>"},{"location":"charts/cnpg-database-manager/#backup-configuration-s3","title":"Backup Configuration (S3)","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    backup:\n      enabled: true\n      schedule: \"0 0 0 * * *\"  # Daily at midnight (6-term cron)\n      retentionPolicy: \"30d\"\n      s3:\n        bucket: my-postgres-backups\n        region: eu-central-1\n        path: /cluster-main\n        accessKeyId: \"\"\n        secretAccessKey: \"\"\n    databases:\n      - name: production-db\n        owner: app\n</code></pre>"},{"location":"charts/cnpg-database-manager/#backup-configuration-azure-blob-storage","title":"Backup Configuration (Azure Blob Storage)","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    backup:\n      enabled: true\n      schedule: \"0 0 0 * * *\"  # Daily at midnight (6-term cron)\n      retentionPolicy: \"30d\"\n      azure:\n        destinationPath: \"https://mystorageaccount.blob.core.windows.net/backups/main\"\n        inheritFromAzureAD: true  # Use Azure AD Workload Identity\n    databases:\n      - name: production-db\n        owner: app\n</code></pre>"},{"location":"charts/cnpg-database-manager/#backup-configuration-google-cloud-storage","title":"Backup Configuration (Google Cloud Storage)","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    backup:\n      enabled: true\n      schedule: \"0 0 0 * * *\"  # Daily at midnight (6-term cron)\n      retentionPolicy: \"30d\"\n      gcs:\n        bucket: my-postgres-backups\n        path: cluster-main\n        gkeEnvironment: true  # Use GKE Workload Identity\n    databases:\n      - name: production-db\n        owner: app\n</code></pre>"},{"location":"charts/cnpg-database-manager/#volume-snapshot-backups","title":"Volume Snapshot Backups","text":"<p>When using <code>method: volumeSnapshot</code> with a <code>schedule</code>, the chart automatically creates a <code>ScheduledBackup</code> CRD that triggers backups according to the cron schedule.</p>"},{"location":"charts/cnpg-database-manager/#important-cron-schedule-format","title":"\u26a0\ufe0f Important: Cron Schedule Format","text":"<p>CloudNativePG uses 6-term cron expressions (not standard 5-term cron):</p> Field Allowed Values Description Seconds 0-59 When to trigger (usually 0) Minutes 0-59 Minute of the hour Hours 0-23 Hour of the day Day 1-31 Day of the month Month 1-12 Month of the year Weekday 0-6 Day of the week (0 = Sunday) <p>Common Examples: - <code>\"0 0 * * * *\"</code> - Every hour at minute 0 - <code>\"0 0 0 * * *\"</code> - Daily at midnight - <code>\"0 30 2 * * *\"</code> - Daily at 2:30 AM - <code>\"0 0 */6 * * *\"</code> - Every 6 hours - <code>\"0 0 2 * * 0\"</code> - Every Sunday at 2:00 AM</p> <p>\u26a0\ufe0f Common Mistake:</p> <p>Do NOT use standard 5-term cron like <code>\"0 * * * *\"</code> - this will be interpreted as running every minute (when seconds=0), not every hour!</p> <p>For more details, see CloudNativePG Backup Documentation.</p> <p>Basic Volume Snapshot Backup:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    backup:\n      enabled: true\n      schedule: \"0 0 */3 * * *\"  # Every 3 hours (6-term cron)\n      retentionPolicy: \"30d\"\n      method: volumeSnapshot\n      volumeSnapshot:\n        className: longhorn-backup  # Your VolumeSnapshotClass\n        online: true\n        onlineConfiguration:\n          immediateCheckpoint: false\n          waitForArchive: true\n    databases:\n      - name: production-db\n        owner: app\n</code></pre> <p>Advanced Configuration with Backup Target:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    backup:\n      enabled: true\n      schedule: \"0 0 */3 * * *\"  # Every 3 hours (6-term cron)\n      method: volumeSnapshot\n      target: prefer-standby  # Use standby for backups (reduces primary load)\n      immediate: true         # Create immediate backup on deployment\n      volumeSnapshot:\n        className: longhorn-backup\n        online: true\n    databases:\n      - name: production-db\n        owner: app\n</code></pre> <p>How it works:</p> <ol> <li>Chart creates the Cluster with <code>backup.volumeSnapshot</code> configuration</li> <li>Chart creates a <code>ScheduledBackup</code> CRD that triggers backups on schedule</li> <li>CloudNativePG operator creates <code>Backup</code> resources automatically</li> <li>Kubernetes CSI creates <code>VolumeSnapshot</code> resources</li> <li>Storage driver (Longhorn, etc.) backs up to configured target (S3/NFS)</li> </ol> <p>Verify backups:</p> <pre><code>kubectl get scheduledbackup\nkubectl get backup\nkubectl get volumesnapshot\n</code></pre>"},{"location":"charts/cnpg-database-manager/#disaster-recovery-and-point-in-time-recovery-pitr","title":"Disaster Recovery and Point-in-Time Recovery (PITR)","text":"<pre><code>clusters:\n  recovered:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 100Gi\n    databases:\n      - name: myapp\n        owner: appuser\n    recovery:\n      enabled: true\n      source: production-cluster\n      recoveryTarget:\n        targetTime: \"2025-01-03T14:30:00Z\"\n        exclusive: false\n    externalClusters:\n      - name: production-cluster\n        barmanObjectStore:\n          destinationPath: \"s3://my-backups/prod-cluster\"\n          endpointURL: \"https://s3.amazonaws.com\"\n          s3Credentials:\n            accessKeyId:\n              name: backup-creds\n              key: ACCESS_KEY_ID\n            secretAccessKey:\n              name: backup-creds\n              key: ACCESS_SECRET_KEY\n          wal:\n            maxParallel: 8\n</code></pre>"},{"location":"charts/cnpg-database-manager/#monitoring-with-prometheus-and-prometheusrule","title":"Monitoring with Prometheus and PrometheusRule","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    monitoring:\n      enabled: true\n      podMonitor:\n        enabled: true\n      prometheusRule:\n        enabled: true\n        labels:\n          team: platform\n        # Optional: Custom alert rules (otherwise defaults are used)\n        rules:\n          - alert: CustomDatabaseSize\n            annotations:\n              summary: \"Database size exceeds threshold\"\n            expr: |\n              cnpg_pg_database_size_bytes &gt; 100000000000\n            for: 10m\n            labels:\n              severity: warning\n    databases:\n      - name: app-db\n        owner: app\n</code></pre>"},{"location":"charts/cnpg-database-manager/#image-catalog-for-centralized-image-management","title":"Image Catalog for Centralized Image Management","text":"<pre><code>imageCatalogs:\n  postgresql:\n    enabled: true\n    kind: ImageCatalog  # or ClusterImageCatalog for cluster-wide\n    images:\n      - major: 15\n        image: ghcr.io/cloudnative-pg/postgresql:15.6\n      - major: 16\n        image: ghcr.io/cloudnative-pg/postgresql:16.2\n      - major: 17\n        image: ghcr.io/cloudnative-pg/postgresql:17.2\n\nclusters:\n  main:\n    enabled: true\n    instances: 3\n    # Use imageCatalogRef instead of imageName for auto-updates\n    imageCatalogRef:\n      apiGroup: postgresql.cnpg.io\n      kind: ImageCatalog\n      name: postgresql\n      major: 17\n    storage:\n      size: 50Gi\n    databases:\n      - name: app-db\n        owner: app\n</code></pre>"},{"location":"charts/cnpg-database-manager/#multiple-databases-in-one-cluster","title":"Multiple Databases in One Cluster","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    databases:\n      - name: keycloak\n        owner: keycloak\n      - name: paperless\n        owner: paperless\n</code></pre>"},{"location":"charts/cnpg-database-manager/#postgresql-extensions-vector-postgis-etc","title":"PostgreSQL Extensions (Vector, PostGIS, etc.)","text":"<p>CloudNativePG supports declarative extension management through the Database CRD (requires CloudNativePG 1.26+). Extensions like <code>pgvector</code>, <code>postgis</code>, and <code>pg_stat_statements</code> are included in the default PostgreSQL images.</p> <p>Simple Extension Setup (pgvector for Immich):</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    databases:\n      - name: immich\n        owner: immich\n        extensions:\n          - name: vector  # pgvector for AI/ML embeddings\n</code></pre> <p>Advanced Extension Configuration:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 100Gi\n    databases:\n      - name: gis-app\n        owner: gis\n        extensions:\n          - name: postgis\n            version: \"3.4.0\"\n            schema: public\n          - name: postgis_topology\n            schema: topology\n\n      - name: timeseries-app\n        owner: timeseries\n        extensions:\n          - name: timescaledb\n          - name: pg_stat_statements\n</code></pre> <p>Common Extensions Available:</p> <ul> <li><code>vector</code> - pgvector for AI/ML embeddings (Immich, ChatGPT apps)</li> <li><code>postgis</code> - Geospatial data support</li> <li><code>timescaledb</code> - Time-series database</li> <li><code>pg_stat_statements</code> - Query performance tracking</li> <li><code>uuid-ossp</code> - UUID generation</li> <li><code>hstore</code> - Key-value storage</li> <li><code>pg_trgm</code> - Trigram matching for fuzzy search</li> </ul> <p>Important Notes:</p> <ul> <li>Extensions are managed per-database (not cluster-wide)</li> <li>CloudNativePG automatically handles <code>CREATE EXTENSION</code> / <code>DROP EXTENSION</code></li> <li>Many extensions are pre-installed in the official CloudNativePG PostgreSQL images</li> <li>For extensions requiring superuser privileges, CloudNativePG handles this automatically</li> <li>Requires CloudNativePG operator version 1.26.0 or higher</li> </ul>"},{"location":"charts/cnpg-database-manager/#separate-wal-storage-performance-optimization","title":"Separate WAL Storage (Performance Optimization)","text":"<p>For performance-critical workloads, you can separate WAL (Write-Ahead Log) storage onto a different volume. This enables parallel I/O and better storage tier optimization.</p> <p>Basic WAL Storage:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n\n    # Main PGDATA storage\n    storage:\n      size: 50Gi\n      storageClass: longhorn-postgres\n\n    # Separate WAL storage\n    walStorage:\n      enabled: true\n      size: 10Gi\n      storageClass: longhorn-postgres-wal  # Can be faster storage tier\n</code></pre> <p>Different Storage Classes:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n\n    # PGDATA on standard storage\n    storage:\n      size: 50Gi\n      storageClass: standard-ssd\n\n    # WAL on faster storage tier\n    walStorage:\n      enabled: true\n      size: 10Gi\n      storageClass: fast-nvme\n</code></pre> <p>Advanced PVC Template Configuration:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n\n    # Advanced PGDATA storage with PVC template\n    storage:\n      pvcTemplate:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 100Gi\n        storageClassName: premium-ssd\n        volumeMode: Filesystem\n\n    # Advanced WAL storage with PVC template\n    walStorage:\n      enabled: true\n      pvcTemplate:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 20Gi\n        storageClassName: ultra-fast-nvme\n        volumeMode: Filesystem\n</code></pre> <p>Important Notes:</p> <ul> <li>WAL storage cannot be removed once added to a cluster</li> <li>Size the WAL volume according to PostgreSQL settings (<code>min_wal_size</code>, <code>max_wal_size</code>)</li> <li>Running out of WAL disk space will halt PostgreSQL</li> <li>Recommended WAL size: 10-20% of PGDATA size</li> </ul>"},{"location":"charts/cnpg-database-manager/#advanced-postgresql-configuration","title":"Advanced PostgreSQL Configuration","text":"<pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 100Gi\n      storageClass: fast-ssd\n    resources:\n      requests:\n        memory: \"4Gi\"\n        cpu: \"2000m\"\n      limits:\n        memory: \"8Gi\"\n        cpu: \"4000m\"\n    postgresql:\n      parameters:\n        max_connections: \"300\"\n        shared_buffers: \"2GB\"\n        effective_cache_size: \"6GB\"\n        maintenance_work_mem: \"512MB\"\n        checkpoint_completion_target: \"0.9\"\n        wal_buffers: \"16MB\"\n        default_statistics_target: \"100\"\n        random_page_cost: \"1.1\"\n        effective_io_concurrency: \"200\"\n        work_mem: \"6990kB\"\n        min_wal_size: \"1GB\"\n        max_wal_size: \"4GB\"\n    databases:\n      - name: highload-app\n        owner: app\n</code></pre>"},{"location":"charts/cnpg-database-manager/#connection-pooling-pgbouncer_1","title":"Connection Pooling (PgBouncer)","text":"<p>CloudNativePG provides built-in connection pooling via PgBouncer through the Pooler CRD. Connection pooling improves database performance and resource utilization by reusing database connections.</p> <p>Basic Pooler Configuration:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    databases:\n      - name: myapp\n        owner: appuser\n\n    # Connection pooling\n    poolers:\n      - name: rw\n        type: rw  # read-write pooler (connects to primary)\n        instances: 3\n        poolMode: transaction  # session or transaction\n        parameters:\n          max_client_conn: \"1000\"\n          default_pool_size: \"25\"\n          min_pool_size: \"5\"\n          reserve_pool_size: \"5\"\n        monitoring:\n          enablePodMonitor: true\n</code></pre> <p>Multi-Pooler Setup (Read-Write + Read-Only):</p> <pre><code>poolers:\n  # Read-write pooler for primary\n  - name: rw\n    type: rw\n    instances: 3\n    poolMode: transaction\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"25\"\n    monitoring:\n      enablePodMonitor: true\n    template:\n      spec:\n        containers:\n        - name: pgbouncer\n          resources:\n            requests:\n              cpu: \"500m\"\n              memory: \"512Mi\"\n            limits:\n              cpu: \"1\"\n              memory: \"1Gi\"\n\n  # Read-only pooler for replicas\n  - name: ro\n    type: ro\n    instances: 5\n    poolMode: transaction\n    parameters:\n      max_client_conn: \"2000\"\n      default_pool_size: \"50\"\n    monitoring:\n      enablePodMonitor: true\n</code></pre> <p>Pooler Types:</p> <ul> <li><code>rw</code> (read-write): Connects to the primary instance for write operations</li> <li><code>ro</code> (read-only): Connects to replica instances for read operations (load balancing)</li> <li><code>r</code> (read-any): Connects to any instance (primary or replicas)</li> </ul> <p>Pool Modes:</p> <ul> <li><code>session</code> (default): Connection held for entire client session. Use when applications need prepared statements, temporary tables, or session variables.</li> <li><code>transaction</code>: Connection released after each transaction. More efficient pooling, ideal for high-throughput OLTP workloads and stateless applications.</li> </ul> <p>Connecting to Pooler:</p> <p>Applications connect to pooler services instead of the database directly:</p> <pre><code># Read-write pooler service\n&lt;cluster-name&gt;-pooler-rw.&lt;namespace&gt;.svc.cluster.local:5432\n\n# Read-only pooler service\n&lt;cluster-name&gt;-pooler-ro.&lt;namespace&gt;.svc.cluster.local:5432\n</code></pre> <p>Advanced Pooler Configuration with additionalPoolerSpec:</p> <p>For Pooler features not explicitly supported by the chart, use <code>additionalPoolerSpec</code>:</p> <pre><code>poolers:\n  - name: rw\n    type: rw\n    instances: 3\n    poolMode: transaction\n    parameters:\n      max_client_conn: \"1000\"\n\n    # Custom service configuration (e.g., LoadBalancer for external access)\n    additionalPoolerSpec:\n      serviceTemplate:\n        metadata:\n          annotations:\n            service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n        spec:\n          type: LoadBalancer\n</code></pre>"},{"location":"charts/cnpg-database-manager/#advanced-role-management","title":"Advanced Role Management","text":"<p>The chart automatically generates PostgreSQL roles for database owners with basic configuration:</p> <pre><code>databases:\n  - name: myapp\n    owner: appuser  # Creates role 'appuser' with login and password\n</code></pre> <p>Auto-generated role attributes: - <code>name</code>: from database owner - <code>ensure: present</code> - <code>login: true</code> - <code>passwordSecret</code>: auto-generated secret</p> <p>For advanced role requirements (superuser, group memberships, connection limits, etc.), use <code>additionalClusterSpec.managed.roles</code>:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 50Gi\n    databases:\n      - name: myapp\n        owner: appuser\n\n    # Advanced role management\n    additionalClusterSpec:\n      managed:\n        roles:\n        # Monitoring user with read-only access\n        - name: monitoring\n          ensure: present\n          login: true\n          inRoles:\n            - pg_monitor\n            - pg_signal_backend\n          comment: \"Prometheus monitoring user\"\n          passwordSecret:\n            name: monitoring-user-password\n\n        # Admin user with superuser privileges\n        - name: admin\n          ensure: present\n          login: true\n          superuser: true\n          connectionLimit: 5\n          validUntil: \"2026-12-31T23:59:59Z\"\n          comment: \"Database administrator\"\n          passwordSecret:\n            name: admin-user-password\n\n        # Application user with connection limit\n        - name: batch_processor\n          ensure: present\n          login: true\n          connectionLimit: 10\n          inherit: true\n          comment: \"Batch processing application\"\n          passwordSecret:\n            name: batch-user-password\n</code></pre> <p>Available role attributes:</p> <ul> <li><code>name</code> (string, required): Role name</li> <li><code>ensure</code> (string): <code>present</code> or <code>absent</code> (default: <code>present</code>)</li> <li><code>login</code> (bool): Allow login (default: <code>true</code>)</li> <li><code>superuser</code> (bool): Superuser privileges (default: <code>false</code>)</li> <li><code>inRoles</code> (array): Group role memberships (e.g., <code>pg_monitor</code>, <code>pg_signal_backend</code>)</li> <li><code>inherit</code> (bool): Inherit privileges from group roles (default: <code>true</code>)</li> <li><code>connectionLimit</code> (int): Maximum concurrent connections (default: <code>-1</code> = unlimited)</li> <li><code>validUntil</code> (string): Password expiration (ISO 8601 format)</li> <li><code>comment</code> (string): Role description/documentation</li> <li><code>passwordSecret</code> (object): Secret reference for password</li> </ul> <p>Important notes:</p> <ul> <li>Auto-generated owner roles are not duplicated if defined in <code>additionalClusterSpec.managed.roles</code></li> <li>Create password secrets manually for custom roles (not auto-generated by chart)</li> <li>Use <code>inRoles</code> for PostgreSQL built-in groups: <code>pg_monitor</code>, <code>pg_signal_backend</code>, <code>pg_read_all_data</code>, <code>pg_write_all_data</code></li> </ul> <p>Example: Create password secret for custom role:</p> <pre><code>kubectl create secret generic monitoring-user-password \\\n  --from-literal=username=monitoring \\\n  --from-literal=password=\"$(openssl rand -base64 24)\" \\\n  --type=kubernetes.io/basic-auth\n</code></pre>"},{"location":"charts/cnpg-database-manager/#advanced-using-additionalclusterspec","title":"Advanced: Using additionalClusterSpec","text":"<p>For CloudNativePG features not explicitly supported by the chart, use <code>additionalClusterSpec</code> to pass any valid CloudNativePG Cluster spec fields.</p> <p>Why Object-Based Configuration?</p> <p>This chart uses an object-based approach (YAML dictionary) for <code>additionalClusterSpec</code> rather than a string-based approach (multiline string). This design choice provides:</p> <ul> <li>\u2705 Type Safety: Schema validation catches configuration errors before deployment</li> <li>\u2705 IDE Support: Autocomplete and validation in modern editors</li> <li>\u2705 Structured API: CloudNativePG has a well-defined API schema (unlike generic Kubernetes resources)</li> <li>\u2705 GitOps Friendly: Proper YAML parsing and diff tools work correctly</li> <li>\u2705 Merge Safety: Helm's YAML processing prevents malformed output</li> </ul> <p>Alternative approaches (like HashiCorp Consul's string-based <code>additionalSpec</code>) are designed for less structured APIs where raw YAML flexibility is needed. For CloudNativePG's typed API, the object-based approach is superior.</p> <p>Usage:</p> <pre><code>clusters:\n  main:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:18\n    storage:\n      size: 100Gi\n    databases:\n      - name: myapp\n        owner: myuser\n\n    # Pass any CloudNativePG Cluster spec field\n    additionalClusterSpec:\n      # Enable superuser access for admin tools\n      enableSuperuserAccess: true\n\n      # Configure pod affinity for multi-zone HA\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: cnpg.io/cluster\n                    operator: In\n                    values: [main]\n              topologyKey: topology.kubernetes.io/zone\n\n      # Custom certificates\n      certificates:\n        serverCASecret: my-ca-secret\n        serverTLSSecret: my-tls-secret\n\n      # Replication tuning\n      minSyncReplicas: 1\n      maxSyncReplicas: 2\n\n      # Separate WAL storage\n      walStorage:\n        size: 50Gi\n        storageClass: fast-nvme\n</code></pre> <p>Important Notes:</p> <ul> <li>Fields in <code>additionalClusterSpec</code> are merged into the Cluster CRD spec using Helm's <code>toYaml</code> function</li> <li>This allows using any CloudNativePG feature without waiting for chart updates</li> <li>The object-based approach provides validation and type safety through the values schema</li> </ul> <p>\u26a0\ufe0f Avoid Overriding Chart-Managed Fields</p> <p>To prevent conflicts, do not define these fields in <code>additionalClusterSpec</code> (use the chart's dedicated parameters instead):</p> Field Use Instead <code>instances</code> Chart parameter: <code>clusters.&lt;name&gt;.instances</code> <code>imageName</code> or <code>imageCatalogRef</code> Chart parameters: <code>clusters.&lt;name&gt;.imageName</code> or <code>clusters.&lt;name&gt;.imageCatalogRef</code> <code>storage</code> Chart parameter: <code>clusters.&lt;name&gt;.storage</code> <code>managed.roles</code> Automatically generated from <code>clusters.&lt;name&gt;.databases</code> <code>postgresql.parameters</code> Chart parameter: <code>clusters.&lt;name&gt;.maxConnections</code>, <code>sharedBuffers</code>, etc. <code>backup</code> Chart parameter: <code>clusters.&lt;name&gt;.backup</code> <code>bootstrap.initdb</code> or <code>bootstrap.recovery</code> Chart parameters: <code>clusters.&lt;name&gt;.initdb.postInitSQL</code> or <code>clusters.&lt;name&gt;.recovery</code> <code>resources</code> Chart parameter: <code>clusters.&lt;name&gt;.resources</code> <code>monitoring</code> Chart parameter: <code>clusters.&lt;name&gt;.monitoring</code> <code>externalClusters</code> Chart parameter: <code>clusters.&lt;name&gt;.externalClusters</code> <p>Best Practices:</p> <ol> <li>Use <code>additionalClusterSpec</code> for fields not exposed by the chart</li> <li>Consult the CloudNativePG API Reference for available fields</li> <li>Test configurations with <code>helm template</code> before applying</li> <li>Document why you're using <code>additionalClusterSpec</code> in your values file comments</li> </ol> <p>Common Use Cases:</p> <ul> <li>High Availability: <code>affinity</code>, <code>topologySpreadConstraints</code></li> <li>Security: <code>enableSuperuserAccess</code>, <code>certificates</code></li> <li>Replication: <code>minSyncReplicas</code>, <code>maxSyncReplicas</code></li> <li>Performance: <code>walStorage</code> (separate WAL disk)</li> <li>Networking: <code>primaryUpdateStrategy</code>, <code>primaryUpdateMethod</code></li> </ul>"},{"location":"charts/cnpg-database-manager/#maintainers","title":"Maintainers","text":"Name Email Url encircle360-oss oss@encircle360.com"},{"location":"charts/cnpg-database-manager/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/encircle360-oss/helm-charts</li> <li>https://cloudnative-pg.io/</li> </ul>"},{"location":"charts/cnpg-database-manager/#values","title":"Values","text":"Key Type Default Description clusters object <code>{}</code> (no clusters deployed by default) PostgreSQL cluster configurations. Each key represents a cluster name. extraObjects list <code>[]</code> Additional Kubernetes objects to deploy (e.g., ExternalSecret, OnePasswordItem for external secret management) imageCatalogs object <code>{}</code> (no catalogs deployed by default) Image catalog configurations. Defines reusable container image references."},{"location":"charts/cnpg-database-manager/#configuration-parameters","title":"Configuration Parameters","text":"<p>The chart uses a dynamic configuration structure where each PostgreSQL cluster is defined under the <code>clusters</code> key. Below are the detailed configuration options:</p>"},{"location":"charts/cnpg-database-manager/#cluster-configuration","title":"Cluster Configuration","text":"Parameter Type Required Default Description <code>clusters.&lt;name&gt;.enabled</code> bool Yes - Enable or disable this cluster <code>clusters.&lt;name&gt;.instances</code> int Yes - Number of PostgreSQL instances (replicas) <code>clusters.&lt;name&gt;.imageName</code> string No* - PostgreSQL container image with tag (*required if imageCatalogRef not set) <code>clusters.&lt;name&gt;.imageCatalogRef</code> object No* - ImageCatalog reference for automatic image management (*alternative to imageName) <code>clusters.&lt;name&gt;.storage.size</code> string Yes - Size of persistent volume (e.g., <code>50Gi</code>) <code>clusters.&lt;name&gt;.storage.storageClass</code> string No - Storage class name <code>clusters.&lt;name&gt;.storage.pvcTemplate</code> object No <code>{}</code> Advanced PVC template for custom storage configuration <code>clusters.&lt;name&gt;.walStorage.enabled</code> bool No <code>false</code> Enable separate WAL storage volume <code>clusters.&lt;name&gt;.walStorage.size</code> string No <code>\"10Gi\"</code> Size of WAL persistent volume <code>clusters.&lt;name&gt;.walStorage.storageClass</code> string No - Storage class name for WAL volume <code>clusters.&lt;name&gt;.walStorage.pvcTemplate</code> object No <code>{}</code> Advanced PVC template for WAL storage <code>clusters.&lt;name&gt;.resources.requests.memory</code> string No - Memory request (e.g., <code>2Gi</code>) <code>clusters.&lt;name&gt;.resources.requests.cpu</code> string No - CPU request (e.g., <code>1000m</code>) <code>clusters.&lt;name&gt;.resources.limits.memory</code> string No - Memory limit (e.g., <code>4Gi</code>) <code>clusters.&lt;name&gt;.resources.limits.cpu</code> string No - CPU limit (e.g., <code>2000m</code>) <code>clusters.&lt;name&gt;.maxConnections</code> string No <code>\"200\"</code> Maximum number of connections <code>clusters.&lt;name&gt;.sharedBuffers</code> string No <code>\"256MB\"</code> Shared memory buffers <code>clusters.&lt;name&gt;.effectiveCacheSize</code> string No <code>\"1GB\"</code> Effective cache size <code>clusters.&lt;name&gt;.maintenanceWorkMem</code> string No <code>\"64MB\"</code> Maintenance work memory <code>clusters.&lt;name&gt;.checkpointCompletionTarget</code> string No <code>\"0.9\"</code> Checkpoint completion target <code>clusters.&lt;name&gt;.walBuffers</code> string No <code>\"16MB\"</code> WAL buffers <code>clusters.&lt;name&gt;.defaultStatisticsTarget</code> string No <code>\"100\"</code> Default statistics target <code>clusters.&lt;name&gt;.randomPageCost</code> string No <code>\"1.1\"</code> Random page cost <code>clusters.&lt;name&gt;.effectiveIoConcurrency</code> string No <code>\"200\"</code> Effective I/O concurrency <code>clusters.&lt;name&gt;.workMem</code> string No <code>\"4MB\"</code> Work memory per operation <code>clusters.&lt;name&gt;.minWalSize</code> string No <code>\"1GB\"</code> Minimum WAL size <code>clusters.&lt;name&gt;.maxWalSize</code> string No <code>\"4GB\"</code> Maximum WAL size <code>clusters.&lt;name&gt;.parameters</code> object No <code>{}</code> Additional custom PostgreSQL parameters <code>clusters.&lt;name&gt;.monitoring.enabled</code> bool No <code>false</code> Enable Prometheus monitoring <code>clusters.&lt;name&gt;.backup.enabled</code> bool No <code>false</code> Enable automated backups <code>clusters.&lt;name&gt;.backup.schedule</code> string No - Backup schedule (cron format, creates ScheduledBackup for volumeSnapshot) <code>clusters.&lt;name&gt;.backup.retentionPolicy</code> string No <code>\"30d\"</code> Backup retention policy <code>clusters.&lt;name&gt;.backup.method</code> string No - Backup method: <code>barmanObjectStore</code> or <code>volumeSnapshot</code> <code>clusters.&lt;name&gt;.backup.target</code> string No - Backup target: <code>primary</code> or <code>prefer-standby</code> <code>clusters.&lt;name&gt;.backup.suspend</code> bool No <code>false</code> Suspend scheduled backups <code>clusters.&lt;name&gt;.backup.immediate</code> bool No <code>false</code> Trigger immediate backup on ScheduledBackup creation <code>clusters.&lt;name&gt;.backup.backupOwnerReference</code> string No - Backup ownership: <code>self</code>, <code>cluster</code>, or <code>none</code> <code>clusters.&lt;name&gt;.backup.volumeSnapshot.className</code> string Yes (if volumeSnapshot) - VolumeSnapshot class name <code>clusters.&lt;name&gt;.backup.volumeSnapshot.online</code> bool No <code>true</code> Online (true) or offline (false) backup <code>clusters.&lt;name&gt;.backup.s3.bucket</code> string Yes (if S3) - S3 bucket name for backups <code>clusters.&lt;name&gt;.backup.s3.endpoint</code> string No - S3 endpoint URL (optional, defaults to AWS) <code>clusters.&lt;name&gt;.backup.s3.region</code> string No - S3 region <code>clusters.&lt;name&gt;.backup.azure.destinationPath</code> string Yes (if Azure) - Azure Blob Storage destination path <code>clusters.&lt;name&gt;.backup.azure.inheritFromAzureAD</code> bool No <code>false</code> Use Azure AD Workload Identity <code>clusters.&lt;name&gt;.backup.gcs.bucket</code> string Yes (if GCS) - GCS bucket name <code>clusters.&lt;name&gt;.backup.gcs.gkeEnvironment</code> bool No <code>false</code> Use GKE Workload Identity <code>clusters.&lt;name&gt;.recovery.enabled</code> bool No <code>false</code> Enable recovery/PITR mode <code>clusters.&lt;name&gt;.recovery.source</code> string No - External cluster name for recovery <code>clusters.&lt;name&gt;.recovery.recoveryTarget.targetTime</code> string No - Point-in-Time Recovery target (RFC 3339) <code>clusters.&lt;name&gt;.poolers[]</code> array No <code>[]</code> Connection pooling configuration (PgBouncer) <code>clusters.&lt;name&gt;.poolers[].name</code> string Yes (if pooler) - Pooler name <code>clusters.&lt;name&gt;.poolers[].type</code> string No <code>\"rw\"</code> Pooler type: <code>rw</code>, <code>ro</code>, or <code>r</code> <code>clusters.&lt;name&gt;.poolers[].poolMode</code> string No <code>\"session\"</code> Pool mode: <code>session</code> or <code>transaction</code> <code>clusters.&lt;name&gt;.poolers[].additionalPoolerSpec</code> object No <code>{}</code> Additional Pooler spec fields <code>clusters.&lt;name&gt;.initdb.postInitSQL</code> array No <code>[]</code> Custom SQL statements after init"},{"location":"charts/cnpg-database-manager/#database-configuration","title":"Database Configuration","text":"Parameter Type Required Default Description <code>clusters.&lt;name&gt;.databases[].name</code> string Yes - Database name (PostgreSQL identifier format) <code>clusters.&lt;name&gt;.databases[].owner</code> string Yes - Database owner/user name (PostgreSQL identifier format) <code>clusters.&lt;name&gt;.databases[].encoding</code> string No <code>\"UTF8\"</code> Database encoding <code>clusters.&lt;name&gt;.databases[].locale</code> string No <code>\"C\"</code> Database locale <code>clusters.&lt;name&gt;.databases[].existingSecret</code> string No - Existing secret for role password (advanced use) <code>clusters.&lt;name&gt;.databases[].extensions[]</code> array No <code>[]</code> PostgreSQL extensions to install (requires CNPG 1.26+) <code>clusters.&lt;name&gt;.databases[].extensions[].name</code> string Yes (if extension) - Extension name (e.g., <code>vector</code>, <code>postgis</code>) <code>clusters.&lt;name&gt;.databases[].extensions[].version</code> string No - Extension version (uses default if not specified) <code>clusters.&lt;name&gt;.databases[].extensions[].schema</code> string No - Schema for extension installation (uses current default)"},{"location":"charts/cnpg-database-manager/#imagecatalog-configuration","title":"ImageCatalog Configuration","text":"Parameter Type Required Default Description <code>imageCatalogs.&lt;name&gt;.enabled</code> bool Yes - Enable or disable this catalog <code>imageCatalogs.&lt;name&gt;.kind</code> string No <code>\"ImageCatalog\"</code> Catalog kind: <code>ImageCatalog</code> or <code>ClusterImageCatalog</code> <code>imageCatalogs.&lt;name&gt;.images[]</code> array Yes - List of PostgreSQL images by major version <code>imageCatalogs.&lt;name&gt;.images[].major</code> int Yes - PostgreSQL major version <code>imageCatalogs.&lt;name&gt;.images[].image</code> string Yes - Container image URL"},{"location":"charts/cnpg-database-manager/#additional-configuration","title":"Additional Configuration","text":"Parameter Type Required Default Description <code>clusters.&lt;name&gt;.additionalClusterSpec</code> object No <code>{}</code> Additional CloudNativePG Cluster spec fields (see Advanced section)"},{"location":"charts/cnpg-database-manager/#how-it-works","title":"How It Works","text":""},{"location":"charts/cnpg-database-manager/#database-and-user-creation","title":"Database and User Creation","text":"<p>For each database defined in <code>clusters.&lt;name&gt;.databases[]</code>, the chart creates:</p> <ol> <li>Managed Roles (in Cluster CRD <code>spec.managed.roles</code>)</li> <li>Each unique <code>owner</code> from all databases becomes a managed PostgreSQL role</li> <li>Roles are automatically deduplicated (multiple databases can share the same owner)</li> <li>CloudNativePG manages the role lifecycle and password rotation</li> <li> <p>Role passwords are stored in secrets: <code>&lt;cluster&gt;-&lt;owner&gt;-password</code></p> </li> <li> <p>Database CRDs (CloudNativePG <code>Database</code> resources)</p> </li> <li>Database name from <code>name</code> field</li> <li>Owner references the managed role created in step 1</li> <li>CloudNativePG operator automatically creates the PostgreSQL database</li> <li> <p>Resource name: <code>&lt;cluster&gt;-&lt;database-name&gt;</code> (sanitized for RFC 1123)</p> </li> <li> <p>Role Password Secrets (Kubernetes Secrets)</p> </li> <li>Type: <code>kubernetes.io/basic-auth</code></li> <li>Name: <code>&lt;cluster&gt;-&lt;owner&gt;-password</code></li> <li>Contains: <code>username</code> and <code>password</code> fields</li> <li>Created in the same namespace as the Helm chart</li> <li>Used by CloudNativePG's managed roles feature</li> </ol>"},{"location":"charts/cnpg-database-manager/#secrets-management","title":"Secrets Management","text":"<p>The chart generates only the secrets required by CloudNativePG for managed roles:</p> <p>Role Password Secrets: <code>&lt;cluster-name&gt;-&lt;owner-name&gt;-password</code> - Example: For cluster <code>main</code> and owner <code>keycloak</code>, the secret is <code>main-keycloak-password</code> - Type: <code>kubernetes.io/basic-auth</code> - Created in the same namespace as the Helm chart - Used by CloudNativePG's <code>spec.managed.roles.passwordSecret</code> - Shared across multiple databases with the same owner</p>"},{"location":"charts/cnpg-database-manager/#application-credentials","title":"Application Credentials","text":"<p>Applications need to create their own secrets in their respective namespaces with database connection details.</p> <p>You can retrieve the password from the role secret and create application secrets manually:</p> <pre><code># Get the password from CNPG namespace\nPASSWORD=$(kubectl -n cnpg-databases get secret main-keycloak-password \\\n  -o jsonpath='{.data.password}' | base64 -d)\n\n# Create application secret in app namespace\nkubectl -n keycloak create secret generic keycloak-db \\\n  --from-literal=username=keycloak \\\n  --from-literal=password=\"$PASSWORD\" \\\n  --from-literal=host=main-rw.cnpg-databases.svc.cluster.local \\\n  --from-literal=port=5432 \\\n  --from-literal=database=keycloak\n</code></pre> <p>For GitOps with SOPS encryption:</p> <pre><code># keycloak/secrets.enc.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: keycloak-db\n  namespace: keycloak\ntype: Opaque\nstringData:\n  username: keycloak\n  password: ENC[AES256_GCM,data:xxx,iv:yyy,tag:zzz,type:str]  # encrypted with sops\n  host: main-rw.cnpg-databases.svc.cluster.local\n  port: \"5432\"\n  database: keycloak\n  # Optional convenience fields\n  jdbc-url: jdbc:postgresql://main-rw.cnpg-databases.svc.cluster.local:5432/keycloak\n  uri: postgresql://keycloak:${password}@main-rw.cnpg-databases.svc.cluster.local:5432/keycloak\n</code></pre> <p>Future: External Secrets Operator Integration</p> <p>For automated secret distribution, consider using External Secrets Operator:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: keycloak-db\n  namespace: keycloak\nspec:\n  secretStoreRef:\n    name: kubernetes-backend\n  target:\n    name: keycloak-db\n  data:\n  - secretKey: password\n    remoteRef:\n      key: main-keycloak-password\n      property: password\n</code></pre>"},{"location":"charts/cnpg-database-manager/#use-cases","title":"Use Cases","text":""},{"location":"charts/cnpg-database-manager/#microservices-consolidation","title":"Microservices Consolidation","text":"<p>Instead of running separate PostgreSQL instances for each microservice:</p> <pre><code>clusters:\n  microservices:\n    enabled: true\n    instances: 3\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 100Gi\n    databases:\n      - name: auth-service\n        owner: auth\n      - name: user-service\n        owner: users\n      - name: order-service\n        owner: orders\n      - name: payment-service\n        owner: payments\n</code></pre>"},{"location":"charts/cnpg-database-manager/#multi-tenant-saas","title":"Multi-Tenant SaaS","text":"<p>Isolate tenant databases while sharing infrastructure:</p> <pre><code>clusters:\n  saas-prod:\n    enabled: true\n    instances: 5\n    imageName: ghcr.io/cloudnative-pg/postgresql:17.2\n    storage:\n      size: 500Gi\n    databases:\n      - name: tenant-acme\n        owner: tenant_acme\n      - name: tenant-widgets\n        owner: tenant_widgets\n      - name: tenant-global\n        owner: tenant_global\n</code></pre>"},{"location":"charts/cnpg-database-manager/#developmentstagingproduction","title":"Development/Staging/Production","text":"<p>Manage all environments from one chart:</p> <pre><code>clusters:\n  dev:\n    enabled: true\n    instances: 1\n    storage:\n      size: 10Gi\n    databases:\n      - name: app-dev\n        owner: dev\n\n  staging:\n    enabled: true\n    instances: 2\n    storage:\n      size: 50Gi\n    databases:\n      - name: app-staging\n        owner: staging\n\n  prod:\n    enabled: true\n    instances: 3\n    storage:\n      size: 200Gi\n    backup:\n      enabled: true\n    databases:\n      - name: app-prod\n        owner: prod\n</code></pre>"},{"location":"charts/cnpg-database-manager/#troubleshooting","title":"Troubleshooting","text":""},{"location":"charts/cnpg-database-manager/#check-cluster-status","title":"Check Cluster Status","text":"<pre><code>kubectl get cluster\nkubectl describe cluster &lt;cluster-name&gt;\n</code></pre>"},{"location":"charts/cnpg-database-manager/#check-database-creation","title":"Check Database Creation","text":"<pre><code>kubectl get database\nkubectl describe database &lt;cluster-name&gt;-&lt;database-name&gt;\n</code></pre>"},{"location":"charts/cnpg-database-manager/#view-generated-secrets","title":"View Generated Secrets","text":"<pre><code>kubectl get secret &lt;cluster-name&gt;-&lt;database-name&gt;-app -o yaml\n</code></pre>"},{"location":"charts/cnpg-database-manager/#check-operator-logs","title":"Check Operator Logs","text":"<pre><code>kubectl logs -n cnpg-system deployment/cnpg-controller-manager\n</code></pre>"},{"location":"charts/cnpg-database-manager/#contributing-maintainership","title":"Contributing &amp; Maintainership","text":""},{"location":"charts/cnpg-database-manager/#we-welcome-contributors","title":"We Welcome Contributors! \ud83c\udf89","text":"<p>We're actively looking for contributors and co-maintainers for this CloudNativePG Database Manager chart! Whether you want to: - Become a co-maintainer for this chart - Submit pull requests for bug fixes, features, or documentation improvements - Help with testing complex PostgreSQL setups (HA, replication, backup/restore) - Improve documentation with production use cases and best practices - Share your PostgreSQL and CloudNativePG expertise with the community</p> <p>Every contribution makes a difference! Whether you're a PostgreSQL DBA, a CloudNativePG user, or a Kubernetes enthusiast - your perspective is valuable.</p>"},{"location":"charts/cnpg-database-manager/#become-a-chart-co-maintainer","title":"Become a Chart Co-Maintainer","text":"<p>Interested in becoming a co-maintainer for this CloudNativePG chart? We'd love to have you!</p> <p>What we're looking for: - Experience with PostgreSQL (administration, replication, backup strategies) - Familiarity with CloudNativePG or interest in learning it - Kubernetes and Helm knowledge - Willingness to review PRs and help with issues - Commitment to helping users achieve reliable PostgreSQL deployments</p> <p>How to get involved: - Start by contributing PRs or helping answer issues/discussions - Reach out to us at oss@encircle360.com expressing your interest - We'll collaborate to onboard you as a co-maintainer</p> <p>We especially welcome PostgreSQL experts who want to share their database knowledge with the Kubernetes community!</p>"},{"location":"charts/cnpg-database-manager/#support-professional-services","title":"Support &amp; Professional Services","text":""},{"location":"charts/cnpg-database-manager/#community-support","title":"Community Support","text":"<p>For issues and questions about this Helm chart: - Open an issue in GitHub Issues - Start a discussion in GitHub Discussions</p> <p>For CloudNativePG specific issues: - Visit the CloudNativePG GitHub repository - Check the CloudNativePG documentation</p>"},{"location":"charts/cnpg-database-manager/#professional-support","title":"Professional Support","text":"<p>For professional support, consulting, custom development, or enterprise solutions, contact hello@encircle360.com</p>"},{"location":"charts/cnpg-database-manager/#disclaimer","title":"Disclaimer","text":"<p>\u26a0\ufe0f This chart is under active development and NOT production-ready.</p> <p>This Helm chart is provided \"AS IS\" without warranty of any kind. encircle360 GmbH and the contributors: - Make no warranties about the completeness, reliability, or accuracy of this chart - Are not liable for any damages arising from the use of this chart - Strongly recommend thorough testing in non-production environments only - Do not recommend this chart for production use at this time</p> <p>Use this chart at your own risk. For production-ready PostgreSQL solutions with SLA requirements, contact our professional support services at hello@encircle360.com</p>"},{"location":"charts/cnpg-database-manager/#license","title":"License","text":"<p>This chart is licensed under the Apache License 2.0. See LICENSE for details.</p>"},{"location":"charts/cnpg-database-manager/#default-values","title":"Default Values","text":"<p>For a complete list of configuration options, see the values.yaml file.</p>"},{"location":"charts/kubevirt/","title":"kubevirt","text":"<p>KubeVirt - Virtual Machine Management on Kubernetes - Deploy and manage VMs as native Kubernetes resources</p> <p>Homepage: https://github.com/encircle360-oss/helm-charts/tree/main/charts/kubevirt</p> <p>\u26a0\ufe0f UNDER CONSTRUCTION This chart is currently under active development and has not been battle-tested in production environments. NOT PRODUCTION READY - Use at your own risk and thoroughly test in non-production environments first. KubeVirt is a complex system requiring deep Kubernetes and virtualization knowledge. Only use this chart if you are an experienced Kubernetes operator.</p>"},{"location":"charts/kubevirt/#description","title":"Description","text":"<p>KubeVirt is a Kubernetes add-on that enables you to run and manage virtual machines alongside container workloads. This Helm chart provides a deployment of KubeVirt v1.7.0 with comprehensive configuration options.</p> <p>Key Features: - Full KubeVirt v1.7.0 support with all feature gates - Operator-based lifecycle management - Comprehensive RBAC configuration - Monitoring integration (ServiceMonitor &amp; PrometheusRule) - High availability configuration - Extensive configuration options</p>"},{"location":"charts/kubevirt/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.32+ (vanilla Kubernetes, K3s, K0s, or OpenShift)</li> <li>Helm 3.8+</li> <li>Nodes with KVM support (hardware virtualization) OR software emulation enabled</li> <li>Sufficient cluster resources for VM workloads</li> </ul> <p>Platform Compatibility: This chart works on both vanilla Kubernetes (including K3s, K0s) and OpenShift. OpenShift-specific RBAC rules are included but will be safely ignored on non-OpenShift clusters.</p> <p>Check Node Virtualization Support: <pre><code># Check if nodes have KVM support\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.allocatable.devices\\.kubevirt\\.io/kvm}{\"\\n\"}{end}'\n\n# Verify KVM is available on nodes\nkubectl debug node/&lt;node-name&gt; -it --image=ubuntu -- bash\napt update &amp;&amp; apt install -y cpu-checker\nkvm-ok\n</code></pre></p>"},{"location":"charts/kubevirt/#installation","title":"Installation","text":""},{"location":"charts/kubevirt/#quick-start","title":"Quick Start","text":"<pre><code># Add the Helm repository\nhelm repo add encircle360-oss https://encircle360-oss.github.io/helm-charts\nhelm repo update\n\n# Install KubeVirt\nhelm install kubevirt encircle360-oss/kubevirt \\\n  --namespace kubevirt \\\n  --create-namespace\n\n# Wait for KubeVirt to be ready\nkubectl wait kubevirt kubevirt -n kubevirt \\\n  --for=condition=Available \\\n  --timeout=10m\n</code></pre>"},{"location":"charts/kubevirt/#installation-with-custom-values","title":"Installation with Custom Values","text":"<pre><code>helm install kubevirt encircle360-oss/kubevirt \\\n  --namespace kubevirt \\\n  --create-namespace \\\n  -f custom-values.yaml\n</code></pre>"},{"location":"charts/kubevirt/#install-with-monitoring","title":"Install with Monitoring","text":"<pre><code># values-monitoring.yaml\nmonitoring:\n  enabled: true\n  namespace: monitoring\n  serviceAccount: prometheus-k8s\n  prometheusRule:\n    enabled: true\n    labels:\n      prometheus: kube-prometheus\n</code></pre> <pre><code>helm install kubevirt encircle360-oss/kubevirt \\\n  -f values-monitoring.yaml \\\n  --namespace kubevirt \\\n  --create-namespace\n</code></pre>"},{"location":"charts/kubevirt/#configuration","title":"Configuration","text":""},{"location":"charts/kubevirt/#basic-configuration","title":"Basic Configuration","text":"Parameter Description Default <code>global.enabled</code> Enable KubeVirt deployment <code>true</code> <code>namespace.name</code> Namespace for KubeVirt <code>kubevirt</code> <code>namespace.create</code> Create the namespace <code>true</code> <code>operator.replicas</code> Number of operator replicas <code>2</code> <code>operator.image.tag</code> Operator image tag <code>v1.7.0</code>"},{"location":"charts/kubevirt/#feature-gates","title":"Feature Gates","text":"<p>KubeVirt uses feature gates to control optional functionality. Feature gates are categorized as: - Alpha: Experimental features that must be explicitly enabled - Beta: Features that are stable but may have limitations - GA: Stable features (enabled by default, no gate needed)</p>"},{"location":"charts/kubevirt/#alphabeta-feature-gates-v170","title":"Alpha/Beta Feature Gates (v1.7.0)","text":"<pre><code>kubevirt:\n  configuration:\n    developerConfiguration:\n      featureGates:\n        # Storage\n        - ExpandDisks              # Dynamic disk expansion\n        - HostDisk                 # Host disk access\n        - ImageVolume              # Image volume support (Beta in v1.7.0)\n\n        # Performance\n        - CPUManager               # CPU pinning and NUMA topology\n        - AlignCPUs                # Align guest and host CPU topology\n\n        # Hardware\n        - HostDevices              # PCI device passthrough\n        - GPUsWithDRA              # GPU with Dynamic Resource Allocation\n        - HostDevicesWithDRA       # Host devices with DRA\n\n        # Security\n        - WorkloadEncryptionSEV    # AMD SEV memory encryption\n        - KubevirtSeccompProfile   # Custom seccomp profiles\n        - SecureExecution          # IBM Secure Execution\n\n        # Networking\n        - VSOCK                    # AF_VSOCK for host-guest communication\n        - PasstIPStackMigration    # PASST IP stack migration\n\n        # Other\n        - ExperimentalIgnitionSupport  # Ignition config support\n        - HypervStrictCheck        # Strict Hyper-V feature checking\n        - Sidecar                  # Sidecar container injection\n        - DownwardMetrics          # Expose metrics to VMs\n        - Root                     # Run virt-launcher as root\n        - DisableMDEVConfiguration # Disable automatic MDEV configuration\n        - PersistentReservation    # SCSI persistent reservations\n        - MultiArchitecture        # Multi-architecture support\n        - VirtIOFSConfigVolumesGate    # VirtioFS for config volumes\n        - VirtIOFSStorageVolumeGate    # VirtioFS for storage volumes\n        - DecentralizedLiveMigration   # Decentralized live migration\n        - ObjectGraph              # Object graph feature\n        - DeclarativeHotplugVolumes    # Declarative hotplug volumes\n        - VideoConfig              # Video device configuration\n        - PanicDevices             # Panic device support\n</code></pre>"},{"location":"charts/kubevirt/#ga-feature-gates-no-configuration-needed","title":"GA Feature Gates (No Configuration Needed)","text":"<p>The following features are stable and enabled by default in v1.7.0. You do NOT need to specify these in feature gates:</p> <ul> <li><code>LiveMigration</code> - Live migration of VMs</li> <li><code>SRIOVLiveMigration</code> - Live migration with SR-IOV</li> <li><code>NonRoot</code> - Run as non-root user</li> <li><code>PSA</code> - Pod Security Admission</li> <li><code>CPUNodeDiscovery</code> - CPU node discovery</li> <li><code>NUMA</code> - NUMA topology support</li> <li><code>GPU</code> - GPU passthrough</li> <li><code>VMLiveUpdateFeatures</code> - Live update VM features (GA in v1.5.0)</li> <li><code>CommonInstancetypesDeploymentGate</code> - Common instance types (GA in v1.4.0)</li> <li><code>HotplugNICs</code> - Hot-plug network interfaces (GA in v1.4.0)</li> <li><code>BochsDisplayForEFIGuests</code> - Bochs display for EFI (GA in v1.4.0)</li> <li><code>AutoResourceLimitsGate</code> - Automatic resource limits (GA in v1.5.0)</li> <li><code>NetworkBindingPlugins</code> - Network binding plugins (GA in v1.5.0)</li> <li><code>DynamicPodInterfaceNaming</code> - Dynamic pod interface naming (GA in v1.5.0)</li> <li><code>VolumesUpdateStrategy</code> - Volumes update strategy (GA in v1.5.0)</li> <li><code>VolumeMigration</code> - Volume migration (GA in v1.5.0)</li> <li><code>InstancetypeReferencePolicy</code> - Instance type reference policy (GA in v1.6.0)</li> <li><code>Snapshot</code> - VM snapshot support (GA in v1.7.0)</li> <li><code>VMExport</code> - VM export functionality (GA in v1.7.0)</li> <li><code>HotplugVolumes</code> - Hot-plug/unplug volumes (GA in v1.7.0)</li> <li><code>NodeRestriction</code> - Node restriction (GA in v1.7.0)</li> </ul>"},{"location":"charts/kubevirt/#cpu-configuration","title":"CPU Configuration","text":"<pre><code>kubevirt:\n  configuration:\n    # Default CPU model for VMs\n    cpuModel: \"host-passthrough\"\n\n    # Default CPU request\n    cpuRequest: \"100m\"\n\n    # Mark obsolete CPUs as unusable\n    obsoleteCPUModels:\n      pentium: true\n      pentium2: true\n      pentium3: true\n      Conroe: true\n      Penryn: true\n</code></pre>"},{"location":"charts/kubevirt/#network-configuration","title":"Network Configuration","text":"<pre><code>kubevirt:\n  configuration:\n    network:\n      # Default network interface type\n      defaultNetworkInterface: \"masquerade\"\n\n      # Allow bridge on pod network\n      permitBridgeInterfaceOnPodNetwork: false\n\n      # Allow SLIRP interface\n      permitSlirpInterface: false\n</code></pre>"},{"location":"charts/kubevirt/#live-migration-configuration","title":"Live Migration Configuration","text":"<pre><code>kubevirt:\n  configuration:\n    migration:\n      # Disable TLS (not recommended for production)\n      disableTLS: false\n\n      # Allow auto-converge for slow migrations\n      allowAutoConverge: false\n\n      # Bandwidth limit per migration\n      bandwidthPerGiB: \"64Mi\"\n\n      # Timeouts\n      completionTimeoutPerGiB: 800\n      progressTimeout: 150\n\n      # Parallelism\n      parallelMigrationsPerCluster: 5\n      parallelOutboundMigrationsPerNode: 2\n\n      # Post-copy migration (use with caution)\n      allowPostCopy: false\n\n      # Dedicated migration network\n      network: \"\"\n</code></pre>"},{"location":"charts/kubevirt/#storage-configuration","title":"Storage Configuration","text":"<pre><code>kubevirt:\n  configuration:\n    # Storage class for VM snapshots and state\n    vmStateStorageClass: \"standard\"\n</code></pre>"},{"location":"charts/kubevirt/#host-device-passthrough","title":"Host Device Passthrough","text":"<pre><code>kubevirt:\n  configuration:\n    permittedHostDevices:\n      # PCI devices\n      pciHostDevices:\n        - pciVendorSelector: \"10DE:1EB8\"  # NVIDIA Tesla T4\n          resourceName: \"nvidia.com/T4\"\n        - pciVendorSelector: \"8086:1572\"  # Intel X710\n          resourceName: \"intel.com/X710\"\n\n      # Mediated devices (vGPU)\n      mediatedDevices:\n        - mdevNameSelector: \"GRID T4-1Q\"\n          resourceName: \"nvidia.com/GRID_T4-1Q\"\n</code></pre>"},{"location":"charts/kubevirt/#node-placement","title":"Node Placement","text":""},{"location":"charts/kubevirt/#infrastructure-components-virt-api-virt-controller","title":"Infrastructure Components (virt-api, virt-controller)","text":"<pre><code>kubevirt:\n  infra:\n    nodePlacement:\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      tolerations:\n        - key: node-role.kubernetes.io/control-plane\n          effect: NoSchedule\n</code></pre>"},{"location":"charts/kubevirt/#workload-components-virt-handler-virt-launcher","title":"Workload Components (virt-handler, virt-launcher)","text":"<pre><code>kubevirt:\n  workloads:\n    nodePlacement:\n      nodeSelector:\n        kubevirt.io/vm-workload: \"true\"\n      tolerations:\n        - key: kubevirt.io/vm-workload\n          effect: NoSchedule\n</code></pre>"},{"location":"charts/kubevirt/#monitoring-configuration","title":"Monitoring Configuration","text":"<pre><code>monitoring:\n  enabled: true\n  namespace: monitoring\n  serviceAccount: prometheus-k8s\n\n  serviceMonitorLabels:\n    prometheus: kube-prometheus\n\n  scrapeInterval: \"30s\"\n\n  prometheusRule:\n    enabled: true\n    labels:\n      prometheus: kube-prometheus\n\n    # Add custom alerting rules\n    additionalRules:\n      - alert: MyCustomAlert\n        expr: up == 0\n        for: 5m\n        labels:\n          severity: critical\n</code></pre>"},{"location":"charts/kubevirt/#usage-examples","title":"Usage Examples","text":""},{"location":"charts/kubevirt/#creating-a-virtual-machine","title":"Creating a Virtual Machine","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/vm: ubuntu-vm\n    spec:\n      domain:\n        devices:\n          disks:\n            - name: containerdisk\n              disk:\n                bus: virtio\n            - name: cloudinitdisk\n              disk:\n                bus: virtio\n        resources:\n          requests:\n            memory: 2Gi\n            cpu: 2\n      volumes:\n        - name: containerdisk\n          containerDisk:\n            image: quay.io/containerdisks/ubuntu:22.04\n        - name: cloudinitdisk\n          cloudInitNoCloud:\n            userData: |\n              #cloud-config\n              password: ubuntu\n              chpasswd: { expire: False }\n              ssh_pwauth: True\n</code></pre>"},{"location":"charts/kubevirt/#using-virtctl","title":"Using virtctl","text":"<pre><code># Install virtctl\nkubectl krew install virt\n\n# Start/Stop VMs\nkubectl virt start ubuntu-vm\nkubectl virt stop ubuntu-vm\nkubectl virt restart ubuntu-vm\n\n# Console access\nkubectl virt console ubuntu-vm\n\n# VNC access\nkubectl virt vnc ubuntu-vm\n\n# SSH into VM (requires SSH service in VM)\nkubectl virt ssh ubuntu@ubuntu-vm\n</code></pre>"},{"location":"charts/kubevirt/#vm-snapshots-requires-snapshot-feature-gate","title":"VM Snapshots (requires Snapshot feature gate)","text":"<pre><code># Enable snapshot feature\nkubevirt:\n  configuration:\n    developerConfiguration:\n      featureGates:\n        - Snapshot\n\n# Create snapshot\napiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: ubuntu-vm-snapshot\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: ubuntu-vm\n</code></pre>"},{"location":"charts/kubevirt/#upgrading","title":"Upgrading","text":""},{"location":"charts/kubevirt/#upgrade-the-chart","title":"Upgrade the Chart","text":"<pre><code>helm repo update\nhelm upgrade kubevirt encircle360-oss/kubevirt \\\n  --namespace kubevirt \\\n  -f values.yaml\n</code></pre>"},{"location":"charts/kubevirt/#upgrade-strategy","title":"Upgrade Strategy","text":"<p>KubeVirt supports rolling updates of VMs during upgrades:</p> <pre><code>kubevirt:\n  workloadUpdateStrategy:\n    workloadUpdateMethods:\n      - LiveMigrate  # Try live migration first\n      - Evict        # Fall back to eviction\n    batchEvictionSize: 10\n    batchEvictionInterval: \"1m\"\n</code></pre>"},{"location":"charts/kubevirt/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"charts/kubevirt/#does-this-work-on-k3sk0svanilla-kubernetes","title":"Does this work on K3s/K0s/vanilla Kubernetes?","text":"<p>Yes! This chart is designed to work on: - Vanilla Kubernetes (upstream) - K3s (lightweight Kubernetes) - K0s (zero friction Kubernetes) - MicroK8s - OpenShift/OKD</p> <p>The chart includes OpenShift-specific RBAC rules, but these are safely ignored on non-OpenShift clusters.</p>"},{"location":"charts/kubevirt/#why-are-there-openshift-resources-in-the-rbac","title":"Why are there OpenShift resources in the RBAC?","text":"<p>KubeVirt officially supports both vanilla Kubernetes and OpenShift. The OpenShift-specific resources (<code>security.openshift.io/securitycontextconstraints</code>, <code>route.openshift.io/routes</code>) are: - Optional and only used on OpenShift - Ignored on vanilla Kubernetes/K3s (API groups don't exist) - Standard practice in the official KubeVirt manifests</p> <p>Your K3s cluster will simply skip these rules - no issues!</p>"},{"location":"charts/kubevirt/#why-do-some-labels-have-empty-values-like-operatorkubevirtio","title":"Why do some labels have empty values like <code>operator.kubevirt.io: \"\"</code>?","text":"<p>This is valid and intentional in Kubernetes! Empty-string label values are used as: - Marker labels: Indicate that something is tagged without needing a specific value - Selector labels: Can be matched with <code>matchLabels: { \"operator.kubevirt.io\": \"\" }</code> - KubeVirt convention: How KubeVirt identifies its own resources internally</p>"},{"location":"charts/kubevirt/#do-i-need-hardware-virtualization-kvm","title":"Do I need hardware virtualization (KVM)?","text":"<p>Recommended but not required: - With KVM (hardware virtualization): Full performance VMs - Without KVM (software emulation): Slower VMs using QEMU emulation</p> <p>Enable software emulation if nodes lack KVM: <pre><code>kubevirt:\n  configuration:\n    developerConfiguration:\n      useEmulation: true\n</code></pre></p>"},{"location":"charts/kubevirt/#can-i-run-this-on-arm64-nodes","title":"Can I run this on ARM64 nodes?","text":"<p>Yes, with limitations. Enable the <code>MultiArchitecture</code> feature gate: <pre><code>kubevirt:\n  configuration:\n    developerConfiguration:\n      featureGates:\n        - MultiArchitecture\n</code></pre></p> <p>Note: Not all VM images support ARM64.</p>"},{"location":"charts/kubevirt/#troubleshooting","title":"Troubleshooting","text":""},{"location":"charts/kubevirt/#check-installation-status","title":"Check Installation Status","text":"<pre><code># Check KubeVirt CR\nkubectl get kubevirt -n kubevirt\n\n# Check all components\nkubectl get pods -n kubevirt\n\n# Check operator logs\nkubectl logs -n kubevirt deployment/virt-operator\n\n# Check virt-handler logs\nkubectl logs -n kubevirt daemonset/virt-handler\n</code></pre>"},{"location":"charts/kubevirt/#common-issues","title":"Common Issues","text":""},{"location":"charts/kubevirt/#1-nodes-without-kvm-support","title":"1. Nodes without KVM support","text":"<p>Error: VMs fail to start with \"KVM not available\"</p> <p>Solution: Enable software emulation (not recommended for production):</p> <pre><code>kubevirt:\n  configuration:\n    developerConfiguration:\n      useEmulation: true\n</code></pre>"},{"location":"charts/kubevirt/#2-vm-fails-to-start-due-to-cpu-model","title":"2. VM fails to start due to CPU model","text":"<p>Error: \"Requested CPU model is not supported\"</p> <p>Solution: Use a more compatible CPU model:</p> <pre><code>kubevirt:\n  configuration:\n    cpuModel: \"host-passthrough\"\n</code></pre>"},{"location":"charts/kubevirt/#3-migration-failures","title":"3. Migration failures","text":"<p>Error: \"Migration failed due to incompatible nodes\"</p> <p>Solution: Ensure nodes have compatible CPU features or use live migration policies.</p>"},{"location":"charts/kubevirt/#debug-commands","title":"Debug Commands","text":"<pre><code># Describe VM\nkubectl describe vm &lt;vm-name&gt;\n\n# Describe VMI (running instance)\nkubectl describe vmi &lt;vm-name&gt;\n\n# Check virt-launcher logs\nkubectl logs &lt;virt-launcher-pod&gt;\n\n# Check events\nkubectl get events -n &lt;namespace&gt; --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"charts/kubevirt/#uninstallation","title":"Uninstallation","text":"<pre><code># Delete all VMs first\nkubectl delete vms --all -A\n\n# Uninstall the chart\nhelm uninstall kubevirt -n kubevirt\n\n# Delete namespace (if desired)\nkubectl delete namespace kubevirt\n</code></pre> <p>Note: By default, CRDs are kept on uninstallation to prevent data loss (<code>helm.sh/resource-policy: keep</code>). To also remove CRDs: <code>kubectl delete crd kubevirts.kubevirt.io</code></p>"},{"location":"charts/kubevirt/#upgrading-from-v01x-to-v020","title":"Upgrading from v0.1.x to v0.2.0","text":"<p>In v0.2.0, CRD management moved from <code>crds/</code> directory (install-only) to regular Helm templates (updated on every upgrade). Existing installations need a one-time CRD adoption before upgrading:</p> <pre><code># Annotate the existing CRD for Helm adoption (one-time, safe, idempotent)\nkubectl annotate crd kubevirts.kubevirt.io \\\n  meta.helm.sh/release-name=kubevirt \\\n  meta.helm.sh/release-namespace=kubevirt \\\n  --overwrite\nkubectl label crd kubevirts.kubevirt.io \\\n  app.kubernetes.io/managed-by=Helm \\\n  --overwrite\n\n# Then upgrade normally\nhelm upgrade kubevirt encircle360-oss/kubevirt -n kubevirt -f values.yaml\n</code></pre> <p>Helmfile users can automate this with a <code>presync</code> hook (see examples in the repository).</p>"},{"location":"charts/kubevirt/#migration-from-raw-manifests","title":"Migration from Raw Manifests","text":"<p>If you're currently using raw KubeVirt manifests:</p> <ol> <li> <p>Export current configuration: <pre><code>kubectl get kubevirt kubevirt -n kubevirt -o yaml &gt; current-config.yaml\n</code></pre></p> </li> <li> <p>Create values.yaml from current config:    Review your current KubeVirt CR and translate settings to Helm values.</p> </li> <li> <p>Uninstall existing KubeVirt: <pre><code>kubectl delete kubevirt kubevirt -n kubevirt\nkubectl delete deployment virt-operator -n kubevirt\n</code></pre></p> </li> <li> <p>Install using Helm: <pre><code>helm install kubevirt encircle360-oss/kubevirt \\\n  -f values.yaml \\\n  --namespace kubevirt\n</code></pre></p> </li> </ol>"},{"location":"charts/kubevirt/#breaking-changes","title":"Breaking Changes","text":""},{"location":"charts/kubevirt/#v16x-v170","title":"v1.6.x \u2192 v1.7.0","text":"<ul> <li>Minimum Kubernetes version: Bumped from 1.30 to 1.32</li> <li>GA Feature Gates: <code>Snapshot</code>, <code>VMExport</code>, <code>HotplugVolumes</code>, and <code>NodeRestriction</code> are now GA. Listing them in featureGates is harmless but unnecessary.</li> <li>Operator scheduling: Default affinity now prefers control-plane nodes. Override with <code>operator.affinity</code> if needed.</li> <li>Instancetype API versions: <code>v1alpha1</code> and <code>v1alpha2</code> instancetype APIs are removed. Migrate to <code>v1beta1</code> or <code>v1</code> before upgrading.</li> </ul>"},{"location":"charts/kubevirt/#v150-v16x","title":"v1.5.0 \u2192 v1.6.x","text":"<ul> <li>VirtualMachineInstanceMigration RBAC: Namespace admins no longer have default permissions to create/edit/delete migrations. Grant explicitly if needed.</li> </ul>"},{"location":"charts/kubevirt/#resources","title":"Resources","text":"<ul> <li>KubeVirt Documentation</li> <li>KubeVirt API Reference</li> <li>Feature Gates Documentation</li> <li>Chart Repository</li> <li>Issue Tracker</li> </ul>"},{"location":"charts/kubevirt/#contributing-maintainership","title":"Contributing &amp; Maintainership","text":""},{"location":"charts/kubevirt/#we-welcome-contributors","title":"We Welcome Contributors! \ud83c\udf89","text":"<p>We're actively seeking contributors and co-maintainers for this KubeVirt chart! Whether you want to: - Become a co-maintainer for this chart - Submit pull requests for bug fixes, features, or documentation improvements - Help with testing virtualization features and VM workloads - Improve documentation with real-world VM deployment examples - Share your virtualization and KubeVirt expertise with the community</p> <p>Your expertise is valuable! Whether you're a virtualization expert, a KubeVirt user, or someone passionate about running VMs on Kubernetes - we'd love your contribution.</p>"},{"location":"charts/kubevirt/#become-a-chart-co-maintainer","title":"Become a Chart Co-Maintainer","text":"<p>Interested in becoming a co-maintainer for this KubeVirt chart? We'd be excited to collaborate!</p> <p>What we're looking for: - Experience with virtualization (KVM, QEMU, libvirt) or KubeVirt - Strong Kubernetes and Helm knowledge - Understanding of VM networking, storage, and live migration - Willingness to review PRs and help with issues - Passion for bringing virtualization to Kubernetes</p> <p>How to get involved: - Start by contributing PRs or helping in issues/discussions - Reach out to us at oss@encircle360.com expressing your interest - We'll work together to onboard you as a co-maintainer</p> <p>We especially welcome virtualization experts who can help users successfully run VMs on Kubernetes!</p>"},{"location":"charts/kubevirt/#support-professional-services","title":"Support &amp; Professional Services","text":""},{"location":"charts/kubevirt/#community-support","title":"Community Support","text":"<p>For issues and questions about this Helm chart: - Open an issue in GitHub Issues - Start a discussion in GitHub Discussions</p> <p>For KubeVirt specific issues: - Visit the KubeVirt GitHub repository - Check the KubeVirt documentation - Join the KubeVirt Slack channel</p>"},{"location":"charts/kubevirt/#professional-support","title":"Professional Support","text":"<p>For professional support, consulting, custom development, or enterprise solutions, contact hello@encircle360.com</p>"},{"location":"charts/kubevirt/#disclaimer","title":"Disclaimer","text":"<p>\u26a0\ufe0f This chart is under active development and NOT production-ready.</p> <p>This Helm chart is provided \"AS IS\" without warranty of any kind. encircle360 GmbH and the contributors: - Make no warranties about the completeness, reliability, or accuracy of this chart - Are not liable for any damages arising from the use of this chart - Strongly recommend thorough testing in non-production environments only - Do not recommend this chart for production use at this time - This chart requires expert-level Kubernetes and virtualization knowledge</p> <p>Use this chart at your own risk. For production-ready virtualization solutions with SLA requirements, contact our professional support services at hello@encircle360.com</p>"},{"location":"charts/kubevirt/#maintainers","title":"Maintainers","text":"Name Email Url encircle360-oss oss@encircle360.com"},{"location":"charts/kubevirt/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/encircle360-oss/helm-charts</li> <li>https://github.com/kubevirt/kubevirt</li> <li>https://kubevirt.io</li> </ul>"},{"location":"charts/kubevirt/#requirements","title":"Requirements","text":"<p>Kubernetes: <code>&gt;=1.32.0-0</code></p>"},{"location":"charts/kubevirt/#values","title":"Values","text":"Key Type Default Description crds object <code>{\"install\":true}</code> CRD configuration crds.install bool <code>true</code> Install CRDs as part of the Helm release (updated on every helm upgrade) global object <code>{\"enabled\":true}</code> Global configuration global.enabled bool <code>true</code> Enable deployment of KubeVirt kubevirt object <code>{\"annotations\":{},\"certificateRotateStrategy\":{\"selfSigned\":{\"caOverlapInterval\":\"168h\",\"caRotateInterval\":\"168h\",\"certRotateInterval\":\"168h\"}},\"configuration\":{\"cpuModel\":\"\",\"cpuRequest\":\"\",\"developerConfiguration\":{\"featureGates\":null,\"useEmulation\":false},\"instancetype\":{\"referencePolicy\":\"reference\"},\"migration\":{\"allowAutoConverge\":false,\"allowPostCopy\":false,\"bandwidthPerGiB\":\"64Mi\",\"completionTimeoutPerGiB\":800,\"disableTLS\":false,\"network\":\"\",\"nodeDrainTaintKey\":\"kubevirt.io/drain\",\"parallelMigrationsPerCluster\":5,\"parallelOutboundMigrationsPerNode\":2,\"progressTimeout\":150,\"unsafeMigrationOverride\":false},\"network\":{\"binding\":{},\"defaultNetworkInterface\":\"masquerade\",\"permitBridgeInterfaceOnPodNetwork\":false,\"permitSlirpInterface\":false},\"obsoleteCPUModels\":{},\"permittedHostDevices\":{\"mediatedDevices\":[],\"pciHostDevices\":[]},\"selinuxLauncherType\":\"virt_launcher.process\",\"smbios\":{},\"vmStateStorageClass\":\"\"},\"deploy\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"\",\"tag\":\"\"},\"infra\":{\"nodePlacement\":{\"affinity\":{},\"nodeSelector\":{},\"tolerations\":[]}},\"labels\":{},\"name\":\"kubevirt\",\"workloadUpdateStrategy\":{\"batchEvictionInterval\":\"1m\",\"batchEvictionSize\":10,\"workloadUpdateMethods\":[]},\"workloads\":{\"nodePlacement\":{\"affinity\":{},\"nodeSelector\":{},\"tolerations\":[]}}}</code> KubeVirt CR configuration kubevirt.annotations object <code>{}</code> Custom annotations for KubeVirt CR kubevirt.certificateRotateStrategy object <code>{\"selfSigned\":{\"caOverlapInterval\":\"168h\",\"caRotateInterval\":\"168h\",\"certRotateInterval\":\"168h\"}}</code> Certificate rotation strategy kubevirt.certificateRotateStrategy.selfSigned.caOverlapInterval string <code>\"168h\"</code> CA overlap interval (duration the old CA is kept) kubevirt.certificateRotateStrategy.selfSigned.caRotateInterval string <code>\"168h\"</code> CA rotation interval kubevirt.certificateRotateStrategy.selfSigned.certRotateInterval string <code>\"168h\"</code> Certificate rotation interval kubevirt.configuration object <code>{\"cpuModel\":\"\",\"cpuRequest\":\"\",\"developerConfiguration\":{\"featureGates\":null,\"useEmulation\":false},\"instancetype\":{\"referencePolicy\":\"reference\"},\"migration\":{\"allowAutoConverge\":false,\"allowPostCopy\":false,\"bandwidthPerGiB\":\"64Mi\",\"completionTimeoutPerGiB\":800,\"disableTLS\":false,\"network\":\"\",\"nodeDrainTaintKey\":\"kubevirt.io/drain\",\"parallelMigrationsPerCluster\":5,\"parallelOutboundMigrationsPerNode\":2,\"progressTimeout\":150,\"unsafeMigrationOverride\":false},\"network\":{\"binding\":{},\"defaultNetworkInterface\":\"masquerade\",\"permitBridgeInterfaceOnPodNetwork\":false,\"permitSlirpInterface\":false},\"obsoleteCPUModels\":{},\"permittedHostDevices\":{\"mediatedDevices\":[],\"pciHostDevices\":[]},\"selinuxLauncherType\":\"virt_launcher.process\",\"smbios\":{},\"vmStateStorageClass\":\"\"}</code> Main configuration section kubevirt.configuration.cpuModel string <code>\"\"</code> Default CPU model for VMs when not specified Example: \"host-passthrough\", \"host-model\", \"Penryn\", \"IvyBridge\", etc. kubevirt.configuration.cpuRequest string <code>\"\"</code> Default CPU request for VMs when not specified Example: \"100m\" or \"1\" (1 core) kubevirt.configuration.developerConfiguration object <code>{\"featureGates\":null,\"useEmulation\":false}</code> Developer configuration kubevirt.configuration.developerConfiguration.featureGates string <code>nil</code> Feature gates configuration Feature gates control optional KubeVirt features See: https://kubevirt.io/user-guide/operations/activating_feature_gates/ kubevirt.configuration.developerConfiguration.useEmulation bool <code>false</code> Use QEMU software emulation instead of KVM hardware virtualization Useful for testing on non-virtualization-capable nodes kubevirt.configuration.instancetype object <code>{\"referencePolicy\":\"reference\"}</code> Instancetype configuration kubevirt.configuration.instancetype.referencePolicy string <code>\"reference\"</code> Reference policy for instance types Options: \"reference\" (default), \"expand\", \"expandAll\" - reference: Store only reference to instancetype - expand: Expand instancetype into VM spec once - expandAll: Always expand instancetype kubevirt.configuration.migration object <code>{\"allowAutoConverge\":false,\"allowPostCopy\":false,\"bandwidthPerGiB\":\"64Mi\",\"completionTimeoutPerGiB\":800,\"disableTLS\":false,\"network\":\"\",\"nodeDrainTaintKey\":\"kubevirt.io/drain\",\"parallelMigrationsPerCluster\":5,\"parallelOutboundMigrationsPerNode\":2,\"progressTimeout\":150,\"unsafeMigrationOverride\":false}</code> Live migration configuration kubevirt.configuration.migration.allowAutoConverge bool <code>false</code> Allow auto-converge for slow migrations kubevirt.configuration.migration.allowPostCopy bool <code>false</code> Allow post-copy migration (use with caution) kubevirt.configuration.migration.bandwidthPerGiB string <code>\"64Mi\"</code> Network bandwidth limit per migration kubevirt.configuration.migration.completionTimeoutPerGiB int <code>800</code> Migration completion timeout per GiB of memory kubevirt.configuration.migration.disableTLS bool <code>false</code> Disable TLS for migrations (not recommended for production) kubevirt.configuration.migration.network string <code>\"\"</code> Dedicated migration network (optional) Example: \"migration-network\" kubevirt.configuration.migration.nodeDrainTaintKey string <code>\"kubevirt.io/drain\"</code> Node drain taint key kubevirt.configuration.migration.parallelMigrationsPerCluster int <code>5</code> Maximum number of parallel migrations in the cluster kubevirt.configuration.migration.parallelOutboundMigrationsPerNode int <code>2</code> Maximum number of parallel outbound migrations per node kubevirt.configuration.migration.progressTimeout int <code>150</code> Progress timeout in seconds kubevirt.configuration.migration.unsafeMigrationOverride bool <code>false</code> Unsafe migration override (allows migrations in unsafe conditions) kubevirt.configuration.network object <code>{\"binding\":{},\"defaultNetworkInterface\":\"masquerade\",\"permitBridgeInterfaceOnPodNetwork\":false,\"permitSlirpInterface\":false}</code> Network configuration kubevirt.configuration.network.binding object <code>{}</code> Network binding plugins configuration kubevirt.configuration.network.defaultNetworkInterface string <code>\"masquerade\"</code> Default network interface type Options: \"masquerade\", \"bridge\", \"slirp\" kubevirt.configuration.network.permitBridgeInterfaceOnPodNetwork bool <code>false</code> Permit bridge interface on pod network kubevirt.configuration.network.permitSlirpInterface bool <code>false</code> Permit SLIRP interface kubevirt.configuration.obsoleteCPUModels object <code>{}</code> Obsolete CPU models that should not be used These CPUs are considered too old and potentially insecure kubevirt.configuration.permittedHostDevices object <code>{\"mediatedDevices\":[],\"pciHostDevices\":[]}</code> Permitted host devices for passthrough kubevirt.configuration.permittedHostDevices.mediatedDevices list <code>[]</code> Mediated devices (vGPU, etc.) kubevirt.configuration.permittedHostDevices.pciHostDevices list <code>[]</code> PCI host devices kubevirt.configuration.selinuxLauncherType string <code>\"virt_launcher.process\"</code> SELinux configuration kubevirt.configuration.smbios object <code>{}</code> SMBIOS configuration (system information exposed to VMs) kubevirt.configuration.vmStateStorageClass string <code>\"\"</code> Storage class for VM state (snapshots, etc.) kubevirt.deploy bool <code>true</code> Deploy KubeVirt Custom Resource (disable for CI tests or when CRDs don't exist yet) kubevirt.image object <code>{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"\",\"tag\":\"\"}</code> Image configuration for KubeVirt components kubevirt.image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy kubevirt.image.registry string <code>\"\"</code> Custom image registry (leave empty to use default quay.io/kubevirt) kubevirt.image.tag string <code>\"\"</code> Custom image tag (leave empty to use operator's default) kubevirt.infra object <code>{\"nodePlacement\":{\"affinity\":{},\"nodeSelector\":{},\"tolerations\":[]}}</code> Infrastructure components placement (virt-api, virt-controller) kubevirt.infra.nodePlacement object <code>{\"affinity\":{},\"nodeSelector\":{},\"tolerations\":[]}</code> Node placement configuration for infrastructure components kubevirt.infra.nodePlacement.affinity object <code>{}</code> Affinity rules kubevirt.infra.nodePlacement.nodeSelector object <code>{}</code> Node selector kubevirt.infra.nodePlacement.tolerations list <code>[]</code> Tolerations kubevirt.labels object <code>{}</code> Custom labels for KubeVirt CR kubevirt.name string <code>\"kubevirt\"</code> Name of the KubeVirt CR kubevirt.workloadUpdateStrategy object <code>{\"batchEvictionInterval\":\"1m\",\"batchEvictionSize\":10,\"workloadUpdateMethods\":[]}</code> Workload update strategy kubevirt.workloadUpdateStrategy.batchEvictionInterval string <code>\"1m\"</code> Interval between batch evictions kubevirt.workloadUpdateStrategy.batchEvictionSize int <code>10</code> Number of VMs to evict in parallel during updates kubevirt.workloadUpdateStrategy.workloadUpdateMethods list <code>[]</code> Workload update methods Options: \"LiveMigrate\", \"Evict\" kubevirt.workloads object <code>{\"nodePlacement\":{\"affinity\":{},\"nodeSelector\":{},\"tolerations\":[]}}</code> Workload components placement (virt-handler, virt-launcher) kubevirt.workloads.nodePlacement object <code>{\"affinity\":{},\"nodeSelector\":{},\"tolerations\":[]}</code> Node placement configuration for workload components kubevirt.workloads.nodePlacement.affinity object <code>{}</code> Affinity rules kubevirt.workloads.nodePlacement.nodeSelector object <code>{}</code> Node selector kubevirt.workloads.nodePlacement.tolerations list <code>[]</code> Tolerations monitoring object <code>{\"enabled\":false,\"namespace\":\"monitoring\",\"prometheusRule\":{\"additionalRules\":[],\"enabled\":false,\"labels\":{}},\"scrapeInterval\":\"30s\",\"serviceAccount\":\"prometheus-k8s\",\"serviceMonitorLabels\":{},\"serviceMonitorNamespace\":\"\"}</code> Monitoring configuration monitoring.enabled bool <code>false</code> Enable monitoring (ServiceMonitor and PrometheusRule) monitoring.namespace string <code>\"monitoring\"</code> Namespace where Prometheus is installed monitoring.prometheusRule object <code>{\"additionalRules\":[],\"enabled\":false,\"labels\":{}}</code> PrometheusRule configuration monitoring.prometheusRule.additionalRules list <code>[]</code> Custom alerting rules Add your custom Prometheus alerting rules here monitoring.prometheusRule.enabled bool <code>false</code> Enable PrometheusRule monitoring.prometheusRule.labels object <code>{}</code> Additional labels for PrometheusRule monitoring.scrapeInterval string <code>\"30s\"</code> Scrape interval for metrics monitoring.serviceAccount string <code>\"prometheus-k8s\"</code> ServiceAccount used by Prometheus monitoring.serviceMonitorLabels object <code>{}</code> Additional labels for ServiceMonitor monitoring.serviceMonitorNamespace string <code>\"\"</code> Namespace for ServiceMonitor resource Leave empty to deploy in the same namespace as KubeVirt namespace object <code>{\"name\":\"kubevirt\"}</code> Namespace configuration namespace.name string <code>\"kubevirt\"</code> Name of the namespace for KubeVirt installation operator object <code>{\"affinity\":{},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"kubevirt/virt-operator\",\"tag\":\"v1.7.0\"},\"imagePullSecrets\":[],\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"kubevirt-cluster-critical\",\"replicas\":2,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"450Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"450Mi\"}},\"tolerations\":[]}</code> KubeVirt Operator configuration operator.affinity object <code>{}</code> Affinity rules for operator pods Default: pod anti-affinity for better distribution across nodes Set to {} to use template defaults or override with custom affinity operator.enabled bool <code>true</code> Enable operator deployment operator.image object <code>{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"kubevirt/virt-operator\",\"tag\":\"v1.7.0\"}</code> Operator container image configuration operator.image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy operator.image.registry string <code>\"quay.io\"</code> Image registry operator.image.repository string <code>\"kubevirt/virt-operator\"</code> Image repository operator.image.tag string <code>\"v1.7.0\"</code> Image tag (defaults to chart appVersion) operator.imagePullSecrets list <code>[]</code> Image pull secrets for private registries operator.nodeSelector object <code>{}</code> Node selector for operator pods (default: kubernetes.io/os: linux) Set to {} to use template defaults or override with custom selectors operator.podAnnotations object <code>{}</code> Pod annotations (e.g., for OpenShift: openshift.io/required-scc: restricted-v2) operator.priorityClassName string <code>\"kubevirt-cluster-critical\"</code> Priority class for operator pods operator.replicas int <code>2</code> Number of operator replicas operator.resources object <code>{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"450Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"450Mi\"}}</code> Resource limits and requests for operator operator.tolerations list <code>[]</code> Tolerations for operator pods (default: CriticalAddonsOnly) Set to [] to use template defaults or override with custom tolerations priorityClass object <code>{\"create\":true,\"description\":\"This priority class should be used for core kubevirt components only.\",\"globalDefault\":false,\"name\":\"kubevirt-cluster-critical\",\"value\":1000000000}</code> Priority class configuration priorityClass.create bool <code>true</code> Create priority class priorityClass.description string <code>\"This priority class should be used for core kubevirt components only.\"</code> Description priorityClass.globalDefault bool <code>false</code> Global default (not recommended for KubeVirt) priorityClass.name string <code>\"kubevirt-cluster-critical\"</code> Priority class name priorityClass.value int <code>1000000000</code> Priority value (higher = more important) rbac object <code>{\"create\":true}</code> RBAC configuration rbac.create bool <code>true</code> Create RBAC resources"},{"location":"charts/roundcube/","title":"roundcube","text":"<p>A free and open source webmail solution with a desktop-like user interface</p> <p>Homepage: https://github.com/encircle360-oss/helm-charts/tree/main/charts/roundcube</p>"},{"location":"charts/roundcube/#introduction","title":"Introduction","text":"<p>This chart bootstraps a Roundcube deployment on a Kubernetes cluster using the Helm package manager.</p> <p>Roundcube is a browser-based multilingual IMAP client with an application-like user interface. It provides full functionality expected from an email client, including MIME support, address book, folder management, message searching and spell checking.</p>"},{"location":"charts/roundcube/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.27+</li> <li>Helm 3.14.0+</li> <li>PV provisioner support in the underlying infrastructure (if using SQLite)</li> <li>Access to an IMAP/SMTP mail server</li> </ul>"},{"location":"charts/roundcube/#installing-the-chart","title":"Installing the Chart","text":"<p>To install the chart with the release name <code>my-roundcube</code>:</p> <pre><code>helm repo add encircle360-oss https://encircle360-oss.github.io/helm-charts/\nhelm repo update\nhelm install my-roundcube encircle360-oss/roundcube\n</code></pre>"},{"location":"charts/roundcube/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To uninstall/delete the <code>my-roundcube</code> deployment:</p> <pre><code>helm uninstall my-roundcube\n</code></pre>"},{"location":"charts/roundcube/#configuration","title":"Configuration","text":""},{"location":"charts/roundcube/#basic-configuration","title":"Basic Configuration","text":"<p>The following table lists the most important configurable parameters:</p> Parameter Description Default <code>roundcube.defaultHost</code> Default IMAP server address <code>ssl://mail.example.com</code> <code>roundcube.defaultPort</code> Default IMAP server port <code>993</code> <code>roundcube.smtpServer</code> SMTP server address <code>tls://mail.example.com</code> <code>roundcube.smtpPort</code> SMTP server port <code>587</code>"},{"location":"charts/roundcube/#database-configuration","title":"Database Configuration","text":"<p>This chart supports three database backends:</p>"},{"location":"charts/roundcube/#sqlite-default","title":"SQLite (Default)","text":"<pre><code>database:\n  type: sqlite\npersistence:\n  enabled: true\n  size: 5Gi\n</code></pre>"},{"location":"charts/roundcube/#mysqlmariadb","title":"MySQL/MariaDB","text":"<pre><code>database:\n  type: mysql\n  external:\n    host: mysql.example.com\n    port: 3306\n    name: roundcube\n    user: roundcube\n    password: secretpassword\n</code></pre>"},{"location":"charts/roundcube/#postgresql","title":"PostgreSQL","text":"<pre><code>database:\n  type: pgsql\n  external:\n    host: postgresql.example.com\n    port: 5432\n    name: roundcube\n    user: roundcube\n    password: secretpassword\n</code></pre>"},{"location":"charts/roundcube/#ingress-configuration","title":"Ingress Configuration","text":"<p>To expose Roundcube via Ingress:</p> <pre><code>ingress:\n  enabled: true\n  className: nginx\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  hosts:\n    - host: webmail.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n    - secretName: roundcube-tls\n      hosts:\n        - webmail.example.com\n</code></pre>"},{"location":"charts/roundcube/#session-storage-with-redis","title":"Session Storage with Redis","text":"<p>For better performance in multi-replica deployments:</p> <pre><code>redis:\n  enabled: true\n  host: redis-master.default.svc.cluster.local\n  port: 6379\n  password: redispassword\n</code></pre>"},{"location":"charts/roundcube/#installing-additional-plugins-via-composer","title":"Installing Additional Plugins via Composer","text":"<p>Roundcube supports installing additional plugins via composer. You can specify plugins using the <code>composerPlugins</code> list:</p> <pre><code>roundcube:\n  plugins:\n    - archive\n    - zipdownload\n  composerPlugins:\n    - johndoh/contextmenu\n    - sblaisot/automatic_addressbook\n    - texxasrulez/persistent_login\n</code></pre> <p>This will: - Install the specified plugins via composer during container startup - Automatically make them available in Roundcube - Work with any plugin available on Packagist</p> <p>Popular composer plugins include: - <code>johndoh/contextmenu</code> - Context menu for message list - <code>sblaisot/automatic_addressbook</code> - Automatically add recipients to address book - <code>texxasrulez/persistent_login</code> - Stay logged in across sessions - <code>weird-birds/thunderbird_labels</code> - Thunderbird-style labels</p>"},{"location":"charts/roundcube/#custom-configuration","title":"Custom Configuration","text":"<p>You can provide custom Roundcube configuration:</p> <pre><code>roundcube:\n  customConfig: |\n    &lt;?php\n    $config['spell_engine'] = 'aspell';\n    $config['enable_spellcheck'] = true;\n    // Additional custom configuration\n</code></pre>"},{"location":"charts/roundcube/#plugin-specific-configuration","title":"Plugin-Specific Configuration","text":"<p>Many Roundcube plugins require their own <code>config.inc.php</code> file in the plugin directory. You can provide plugin-specific configurations using the <code>pluginConfigs</code> map:</p> <pre><code>roundcube:\n  plugins:\n    - identity_from_directory\n    - persistent_login\n  pluginConfigs:\n    identity_from_directory: |\n      &lt;?php\n      $config['identity_from_directory_ldap_host'] = ['ldap://localhost:389'];\n      $config['identity_from_directory_ldap_base_dn'] = 'dc=example,dc=com';\n      $config['identity_from_directory_ldap_bind_dn'] = 'cn=admin,dc=example,dc=com';\n      $config['identity_from_directory_ldap_bind_pass'] = 'secret';\n    persistent_login: |\n      &lt;?php\n      $config['login_lifetime'] = 30;\n      $config['login_secure_cookie'] = true;\n      $config['login_token_expiration'] = 3600;\n</code></pre> <p>This will: - Create a separate ConfigMap for each plugin configuration - Mount each config directly to <code>/var/www/html/plugins/&lt;plugin-name&gt;/config.inc.php</code> - No init containers or lifecycle hooks required - Configs are immediately available when the container starts</p>"},{"location":"charts/roundcube/#plugin-specific-configuration-with-secrets","title":"Plugin-Specific Configuration with Secrets","text":"<p>For sensitive plugin configuration values (passwords, API tokens, OAuth secrets), use the <code>secretEnvVars</code> feature for a fully declarative approach:</p> <p>Option 1: Using secretEnvVars (Recommended - Fully Declarative)</p> <pre><code>roundcube:\n  plugins:\n    - identity_from_directory\n  pluginConfigs:\n    identity_from_directory: |\n      &lt;?php\n      $config['identity_from_directory_ldap_host'] = ['ldap://localhost:389'];\n      $config['identity_from_directory_ldap_base_dn'] = 'dc=example,dc=com';\n      $config['identity_from_directory_ldap_bind_dn'] = 'cn=admin,dc=example,dc=com';\n      $config['identity_from_directory_ldap_bind_pass'] = getenv('LDAP_BIND_PASSWORD');\n  secretEnvVars:\n    LDAP_BIND_PASSWORD: \"my-secret-password\"\n    API_KEY: \"my-api-key\"\n    OAUTH_CLIENT_SECRET: \"oauth-secret\"\n</code></pre> <p>This will: - Automatically create a Kubernetes Secret - Inject environment variables into the container - Work with SOPS/sealed-secrets by encrypting your <code>values.yaml</code> or separate <code>secrets.yaml</code> - No external Secret creation or Helmfile hooks needed</p> <p>Security with SOPS:</p> <pre><code># Encrypt your values file with SOPS\nsops -e values.yaml &gt; values.enc.yaml\n\n# Deploy with encrypted values\nhelm secrets upgrade my-release ./roundcube -f values.enc.yaml\n</code></pre> <p>Option 2: Using External Secrets (For existing secret management solutions)</p> <pre><code>roundcube:\n  pluginConfigs:\n    identity_from_directory: |\n      &lt;?php\n      $config['identity_from_directory_ldap_bind_pass'] = getenv('LDAP_BIND_PASSWORD');\n  extraEnvVars:\n    - name: LDAP_BIND_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-existing-secret\n          key: password\n</code></pre> <p>You must create the Secret separately (manually or via external-secrets operator):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-existing-secret\ntype: Opaque\nstringData:\n  password: my-secret-password\n</code></pre> <p>Security Benefits: - <code>secretEnvVars</code>: Fully declarative, works with SOPS/sealed-secrets, no external tools needed - <code>extraEnvVars</code>: Supports external secret management (External Secrets Operator, Vault, etc.) - Secrets separated from configuration - Secrets never stored in ConfigMaps - Environment variables accessible via <code>getenv()</code> in PHP</p>"},{"location":"charts/roundcube/#multi-domain-support","title":"Multi-Domain Support","text":"<p>Roundcube can serve multiple domains with completely different configurations:</p> <pre><code>roundcube:\n  multiDomain:\n    enabled: true\n    domains:\n      - name: \"mail.example.com\"\n        config: |\n          $config['default_host'] = 'ssl://imap.example.com:993';\n          $config['smtp_server'] = 'tls://smtp.example.com:587';\n          $config['managesieve_host'] = 'imap.example.com';\n          $config['managesieve_port'] = 4190;\n          $config['plugins'] = array('managesieve', 'password', 'archive');\n      - name: \"mail.company.org\"\n        config: |\n          $config['default_host'] = 'ssl://mail.company.org:993';\n          $config['smtp_server'] = 'ssl://mail.company.org:465';\n          $config['smtp_conn_options'] = array(\n            'ssl' =&gt; array('verify_peer' =&gt; false)\n          );\n          $config['password_algorithm'] = 'sha256-crypt';\n          $config['password_db_dsn'] = 'mysql://user:pass@localhost/users';\n          $config['plugins'] = array('password', 'vacation', 'forward');\n\ningress:\n  enabled: true\n  hosts:\n    - host: mail.example.com\n      paths:\n        - path: /\n    - host: mail.company.org\n      paths:\n        - path: /\n</code></pre> <p>With this configuration: - Each domain can have completely different settings - Full control over IMAP, SMTP, ManageSieve, OAuth, plugins, etc. - Users accessing different domains get their specific configuration - Fallback to default settings if domain is not configured</p>"},{"location":"charts/roundcube/#maintainers","title":"Maintainers","text":"Name Email Url encircle360-oss oss@encircle360.com"},{"location":"charts/roundcube/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/roundcube/roundcubemail</li> <li>https://github.com/encircle360-oss/helm-charts</li> </ul>"},{"location":"charts/roundcube/#values","title":"Values","text":"Key Type Default Description affinity object <code>{}</code> autoscaling.enabled bool <code>false</code> autoscaling.maxReplicas int <code>10</code> autoscaling.minReplicas int <code>1</code> autoscaling.targetCPUUtilizationPercentage int <code>80</code> autoscaling.targetMemoryUtilizationPercentage int <code>80</code> database.external.existingSecret string <code>\"\"</code> database.external.existingSecretPasswordKey string <code>\"password\"</code> database.external.existingSecretUserKey string <code>\"username\"</code> database.external.host string <code>\"\"</code> database.external.name string <code>\"roundcube\"</code> database.external.password string <code>\"\"</code> database.external.port int <code>3306</code> database.external.user string <code>\"roundcube\"</code> database.type string <code>\"sqlite\"</code> fullnameOverride string <code>\"\"</code> image.pullPolicy string <code>\"IfNotPresent\"</code> image.repository string <code>\"roundcube/roundcubemail\"</code> image.tag string <code>\"\"</code> imagePullSecrets list <code>[]</code> ingress.annotations object <code>{}</code> ingress.className string <code>\"\"</code> ingress.enabled bool <code>false</code> ingress.hosts[0].host string <code>\"webmail.example.com\"</code> ingress.hosts[0].paths[0].path string <code>\"/\"</code> ingress.hosts[0].paths[0].pathType string <code>\"Prefix\"</code> ingress.tls list <code>[]</code> initContainers list <code>[]</code> livenessProbe.enabled bool <code>true</code> livenessProbe.failureThreshold int <code>3</code> livenessProbe.initialDelaySeconds int <code>30</code> livenessProbe.periodSeconds int <code>10</code> livenessProbe.successThreshold int <code>1</code> livenessProbe.timeoutSeconds int <code>5</code> memcached.enabled bool <code>false</code> memcached.host string <code>\"\"</code> memcached.port int <code>11211</code> nameOverride string <code>\"\"</code> nodeSelector object <code>{}</code> persistence.accessMode string <code>\"ReadWriteOnce\"</code> persistence.enabled bool <code>true</code> persistence.existingClaim string <code>\"\"</code> persistence.logsPath string <code>\"/var/roundcube/logs\"</code> persistence.size string <code>\"5Gi\"</code> persistence.sqlitePath string <code>\"/var/roundcube/db\"</code> persistence.storageClass string <code>\"\"</code> persistence.tempPath string <code>\"/tmp/roundcube\"</code> podAnnotations object <code>{}</code> podSecurityContext.fsGroup int <code>33</code> podSecurityContext.runAsNonRoot bool <code>true</code> podSecurityContext.runAsUser int <code>33</code> readinessProbe.enabled bool <code>true</code> readinessProbe.failureThreshold int <code>3</code> readinessProbe.initialDelaySeconds int <code>5</code> readinessProbe.periodSeconds int <code>10</code> readinessProbe.successThreshold int <code>1</code> readinessProbe.timeoutSeconds int <code>5</code> redis.enabled bool <code>false</code> redis.existingSecret string <code>\"\"</code> redis.existingSecretPasswordKey string <code>\"password\"</code> redis.host string <code>\"\"</code> redis.password string <code>\"\"</code> redis.port int <code>6379</code> replicaCount int <code>1</code> resources.limits.memory string <code>\"512Mi\"</code> resources.requests.cpu string <code>\"100m\"</code> resources.requests.memory string <code>\"256Mi\"</code> roundcube.composerPlugins list <code>[]</code> roundcube.customConfig string <code>\"\"</code> roundcube.defaultHost string <code>\"ssl://mail.example.com\"</code> roundcube.defaultPort int <code>993</code> roundcube.desKey string <code>\"\"</code> roundcube.extraEnvVars list <code>[]</code> roundcube.extraVolumeMounts list <code>[]</code> roundcube.extraVolumes list <code>[]</code> roundcube.multiDomain.domains list <code>[]</code> roundcube.multiDomain.enabled bool <code>false</code> roundcube.pluginConfigs object <code>{}</code> roundcube.plugins[0] string <code>\"archive\"</code> roundcube.plugins[1] string <code>\"zipdownload\"</code> roundcube.plugins[2] string <code>\"managesieve\"</code> roundcube.productName string <code>\"Roundcube Webmail\"</code> roundcube.secretEnvVars object <code>{}</code> roundcube.skin string <code>\"elastic\"</code> roundcube.smtpPass string <code>\"%p\"</code> roundcube.smtpPort int <code>587</code> roundcube.smtpServer string <code>\"tls://mail.example.com\"</code> roundcube.smtpUser string <code>\"%u\"</code> roundcube.supportUrl string <code>\"\"</code> roundcube.uploadMaxFilesize string <code>\"25M\"</code> securityContext.allowPrivilegeEscalation bool <code>false</code> securityContext.capabilities.drop[0] string <code>\"ALL\"</code> securityContext.readOnlyRootFilesystem bool <code>false</code> service.annotations object <code>{}</code> service.port int <code>80</code> service.targetPort int <code>80</code> service.type string <code>\"ClusterIP\"</code> serviceAccount.annotations object <code>{}</code> serviceAccount.create bool <code>true</code> serviceAccount.name string <code>\"\"</code> sidecarContainers list <code>[]</code> startupProbe.enabled bool <code>false</code> startupProbe.failureThreshold int <code>30</code> startupProbe.initialDelaySeconds int <code>0</code> startupProbe.periodSeconds int <code>10</code> startupProbe.successThreshold int <code>1</code> startupProbe.timeoutSeconds int <code>5</code> tolerations list <code>[]</code>"},{"location":"charts/roundcube/#examples","title":"Examples","text":""},{"location":"charts/roundcube/#minimal-configuration-with-sqlite","title":"Minimal Configuration with SQLite","text":"<pre><code>roundcube:\n  defaultHost: \"ssl://imap.gmail.com\"\n  defaultPort: 993\n  smtpServer: \"tls://smtp.gmail.com\"\n  smtpPort: 587\n\ndatabase:\n  type: sqlite\n\npersistence:\n  enabled: true\n</code></pre>"},{"location":"charts/roundcube/#production-configuration-with-external-database","title":"Production Configuration with External Database","text":"<pre><code>replicaCount: 3\n\nroundcube:\n  defaultHost: \"ssl://mail.company.com\"\n  smtpServer: \"tls://mail.company.com\"\n\ndatabase:\n  type: mysql\n  external:\n    host: mysql.company.local\n    name: roundcube_prod\n    user: roundcube\n    existingSecret: roundcube-db-secret\n\nredis:\n  enabled: true\n  host: redis.company.local\n\ningress:\n  enabled: true\n  className: nginx\n  hosts:\n    - host: webmail.company.com\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n    - secretName: webmail-tls\n      hosts:\n        - webmail.company.com\n\nresources:\n  requests:\n    cpu: 200m\n    memory: 256Mi\n  limits:\n    cpu: 1000m\n    memory: 1Gi\n\nautoscaling:\n  enabled: true\n  minReplicas: 3\n  maxReplicas: 10\n</code></pre>"},{"location":"charts/roundcube/#troubleshooting","title":"Troubleshooting","text":""},{"location":"charts/roundcube/#logs","title":"Logs","text":"<p>View Roundcube logs:</p> <pre><code>kubectl logs -l app.kubernetes.io/name=roundcube\n</code></pre>"},{"location":"charts/roundcube/#database-connection-issues","title":"Database Connection Issues","text":"<p>If you're experiencing database connection issues:</p> <ol> <li>Verify database credentials</li> <li>Check network connectivity to database host</li> <li>Ensure database exists and user has proper permissions</li> </ol>"},{"location":"charts/roundcube/#session-issues","title":"Session Issues","text":"<p>If users are getting logged out frequently:</p> <ol> <li>Enable Redis for session storage</li> <li>Ensure all replicas can access the same session store</li> <li>Check session timeout settings</li> </ol>"},{"location":"charts/roundcube/#contributing-maintainership","title":"Contributing &amp; Maintainership","text":""},{"location":"charts/roundcube/#we-welcome-contributors","title":"We Welcome Contributors! \ud83c\udf89","text":"<p>We're looking for contributors and co-maintainers for this Roundcube chart! Whether you want to: - Become a co-maintainer for this chart - Submit pull requests for bug fixes, features, or documentation improvements - Help with testing Roundcube deployments in different environments - Improve documentation with real-world examples and best practices - Share your Roundcube expertise with the community</p> <p>Every contribution is valuable! You don't need to be a Helm expert - if you know Roundcube well and want to help others deploy it on Kubernetes, we'd love your input.</p>"},{"location":"charts/roundcube/#become-a-chart-co-maintainer","title":"Become a Chart Co-Maintainer","text":"<p>Interested in becoming a co-maintainer for this Roundcube chart? We'd be thrilled!</p> <p>What we're looking for: - Experience with Roundcube (configuration, plugins, administration) - Interest in Kubernetes and Helm - Willingness to help review PRs and answer issues - Passion for helping others deploy Roundcube successfully</p> <p>How to get involved: - Start by contributing PRs or helping in issues/discussions - Reach out to us at oss@encircle360.com expressing your interest - We'll work together to onboard you as a co-maintainer</p> <p>You don't need to commit full-time - even occasional help is valuable and appreciated!</p>"},{"location":"charts/roundcube/#support-professional-services","title":"Support &amp; Professional Services","text":""},{"location":"charts/roundcube/#community-support","title":"Community Support","text":"<p>For issues and questions about this Helm chart: - Open an issue in GitHub Issues - Start a discussion in GitHub Discussions</p> <p>For Roundcube specific issues: - Visit the Roundcube GitHub repository - Check the Roundcube documentation</p>"},{"location":"charts/roundcube/#professional-support","title":"Professional Support","text":"<p>For professional support, consulting, custom development, or enterprise solutions, contact hello@encircle360.com</p>"},{"location":"charts/roundcube/#disclaimer","title":"Disclaimer","text":"<p>This Helm chart is provided \"AS IS\" without warranty of any kind. encircle360 GmbH and the contributors: - Make no warranties about the completeness, reliability, or accuracy of this chart - Are not liable for any damages arising from the use of this chart - Recommend thorough testing in non-production environments before production use</p> <p>Use this chart at your own risk. For production deployments with SLA requirements, consider our professional support services.</p>"},{"location":"charts/roundcube/#license","title":"License","text":"<p>This chart is licensed under the Apache License 2.0. See LICENSE for details.</p>"},{"location":"charts/yaade/","title":"yaade","text":""},{"location":"charts/yaade/#installation","title":"Installation","text":"<pre><code>helm repo add encircle360 https://encircle360-oss.github.io/helm-charts\nhelm repo update\nhelm install my-yaade encircle360/yaade\n</code></pre> <p>Yaade is an open-source, self-hosted, collaborative API development environment</p> <p>Homepage: https://github.com/encircle360-oss/helm-charts/tree/main/charts/yaade</p>"},{"location":"charts/yaade/#maintainers","title":"Maintainers","text":"Name Email Url encircle360-oss oss@encircle360.com"},{"location":"charts/yaade/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/encircle360-oss/helm-charts</li> <li>https://github.com/EsperoTech/yaade</li> <li>https://docs.yaade.io</li> </ul>"},{"location":"charts/yaade/#values","title":"Values","text":"Key Type Default Description affinity object <code>{}</code> Affinity autoscaling object <code>{\"enabled\":false,\"maxReplicas\":5,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":80,\"targetMemoryUtilizationPercentage\":80}</code> Autoscaling configuration (note: requires shared storage for multiple replicas) extraVolumeMounts list <code>[]</code> Extra volume mounts extraVolumes list <code>[]</code> Extra volumes fileStorage object <code>{\"accessMode\":\"ReadWriteOnce\",\"annotations\":{},\"enabled\":false,\"existingClaim\":\"\",\"path\":\"\",\"size\":\"20Gi\",\"storageClass\":\"\"}</code> File storage configuration Files uploaded in Yaade are stored separately from the database IMPORTANT: Files are NOT included in database backups! fileStorage.accessMode string <code>\"ReadWriteOnce\"</code> Access mode for file storage fileStorage.annotations object <code>{}</code> Annotations for file storage PVC fileStorage.enabled bool <code>false</code> Enable separate persistent volume for file storage If disabled, files are stored in the main data volume at /app/data/files fileStorage.existingClaim string <code>\"\"</code> Existing claim for file storage fileStorage.path string <code>\"\"</code> Custom file storage path (sets YAADE_FILE_STORAGE_PATH) Leave empty to use default location fileStorage.size string <code>\"20Gi\"</code> Size of file storage volume fileStorage.storageClass string <code>\"\"</code> Storage class for file storage volume fullnameOverride string <code>\"\"</code> Override full name image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy image.repository string <code>\"esperotech/yaade\"</code> Image repository image.tag string <code>\"\"</code> Overrides the image tag whose default is the chart appVersion imagePullSecrets list <code>[]</code> Image pull secrets ingress.annotations object <code>{}</code> Annotations ingress.className string <code>\"\"</code> Ingress class name ingress.enabled bool <code>false</code> Enable ingress ingress.hosts list <code>[{\"host\":\"yaade.example.com\",\"paths\":[{\"path\":\"/\",\"pathType\":\"Prefix\"}]}]</code> Hosts configuration ingress.tls list <code>[]</code> TLS configuration initContainers list <code>[]</code> Init containers livenessProbe object <code>{\"enabled\":true,\"failureThreshold\":3,\"httpGet\":{\"path\":\"/\",\"port\":\"http\"},\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5}</code> Liveness probe configuration nameOverride string <code>\"\"</code> Override name nodeSelector object <code>{}</code> Node selector persistence object <code>{\"accessMode\":\"ReadWriteOnce\",\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"size\":\"10Gi\",\"storageClass\":\"\"}</code> Persistence configuration for Yaade data persistence.accessMode string <code>\"ReadWriteOnce\"</code> Access mode persistence.annotations object <code>{}</code> Annotations for PVC persistence.enabled bool <code>true</code> Enable persistence for /app/data (H2 database) persistence.existingClaim string <code>\"\"</code> Existing claim persistence.size string <code>\"10Gi\"</code> Size persistence.storageClass string <code>\"\"</code> Storage class podAnnotations object <code>{}</code> Pod annotations podSecurityContext object <code>{\"fsGroup\":1000,\"runAsNonRoot\":true,\"runAsUser\":1000}</code> Pod security context readinessProbe object <code>{\"enabled\":true,\"failureThreshold\":3,\"httpGet\":{\"path\":\"/\",\"port\":\"http\"},\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5}</code> Readiness probe configuration replicaCount int <code>1</code> Number of replicas (note: multiple replicas require shared storage) resources object <code>{\"limits\":{\"memory\":\"512Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"256Mi\"}}</code> Resource limits and requests securityContext object <code>{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":false}</code> Security context service.annotations object <code>{}</code> Annotations service.port int <code>9339</code> Service port service.targetPort int <code>9339</code> Target port service.type string <code>\"ClusterIP\"</code> Service type serviceAccount.annotations object <code>{}</code> Annotations to add to the service account serviceAccount.create bool <code>true</code> Specifies whether a service account should be created serviceAccount.name string <code>\"\"</code> The name of the service account to use sidecarContainers list <code>[]</code> Sidecar containers startupProbe object <code>{\"enabled\":true,\"failureThreshold\":30,\"httpGet\":{\"path\":\"/\",\"port\":\"http\"},\"initialDelaySeconds\":0,\"periodSeconds\":5,\"successThreshold\":1,\"timeoutSeconds\":3}</code> Startup probe configuration tolerations list <code>[]</code> Tolerations yaade object <code>{\"adminUsername\":\"admin\",\"basePath\":\"\",\"extraEnv\":[],\"fileStoragePath\":\"\"}</code> Yaade specific configuration yaade.adminUsername string <code>\"admin\"</code> Admin username for initial setup This is required on first startup. Default password is \"password\" and must be changed after first login. yaade.basePath string <code>\"\"</code> Base path for reverse proxy deployments Set this when running Yaade behind a reverse proxy that serves it at a subpath (e.g., /yaade) Example: \"/yaade\" for https://example.com/yaade yaade.extraEnv list <code>[]</code> Additional environment variables Use this for any additional environment variables needed by Yaade yaade.fileStoragePath string <code>\"\"</code> File storage path (YAADE_FILE_STORAGE_PATH) Set this to customize where uploaded files are stored Leave empty to use default location (/app/data/files) Can be used with network storage (NFS, s3fs, etc.) <p>Autogenerated from chart metadata using helm-docs v1.14.2</p>"},{"location":"charts/yaade/#default-values","title":"Default Values","text":"<p>For a complete list of configuration options, see the values.yaml file.</p>"}]}